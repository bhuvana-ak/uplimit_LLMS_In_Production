{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8f9839ac8bd844bd99bdc9a43c03b3ab": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_76ce4532240045e4b1716d66cc915fe8",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[38;2;106;0;255m⠧\u001b[0m ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness (GEval) Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠧</span> ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness (GEval) Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "76ce4532240045e4b1716d66cc915fe8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhuvana-ak/uplimit_LLMS_In_Production/blob/main/LLMS_In_Production_W1_Project_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataAlchemy Labs\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1jLQB9IRtp4cHnIxRSebSVcclKxNfdHCf\" />\n",
        "\n",
        "Welcome to the week 1 project for LLMs in Production. In this weeks project you are a founding Machine Learning Engineer on the team you and your team have narrowed down a life-changing product. You will be building and launching your groundbreaking SaaS product which is quite similar to a product already out in the market, [AI2sql](https://www.ai2sql.io/)."
      ],
      "metadata": {
        "id": "j8srXs0p12CE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![AI2sql Example](https://drive.google.com/uc?id=1mWOQs6OZmuCLui-auLosXsXGhwl2mbVg)\n",
        "\n",
        "You have done a significant amount of market research and are confident that this product will gather a cult-like following with a killer feature of being able to take any natural language query and output SQL code based on the query.\n",
        "\n",
        "Potential users of your application are:\n",
        "- **Project Managers**: They can use our tool to quickly derive important metrics for presentations or reports. Instead of relying on technical teams, they can directly query the database using natural language.\n",
        "- **Data Scientists**: Often dealing with complex data queries, data scientists can use our tool to streamline and debug intricate SQL statements, saving time and reducing the potential for error.\n",
        "- **Business Analysts and Non-Tech Professionals**: Anyone without deep SQL knowledge but needing to interact with the database can benefit immensely. They can easily convert their data needs into SQL queries without delving into the complexities of SQL syntax.\n",
        "\n",
        "There are many more features that you can work on in the future such as:\n",
        "1. NoSQL Code Generation\n",
        "2. SQL Syntax Checking\n",
        "3. Explaining SQL Code\n",
        "4. Optimizing SQL Code\n",
        "5. Formatting SQL Code\n",
        "\n",
        "But since this is an MVP and you want to laucnh as quickly as possible we will be just focussing on the use case of SQL Code Generation. In particular we will be supporting the use of **SQL** initially as its the most popular database out there in use currently based on the recent [StackOverflow Developer Survey 2023](https://survey.stackoverflow.co/2023/#section-most-popular-technologies-databases).\n",
        "\n",
        "\n",
        "\n",
        "Our final product will be a simple web application with a basic Q/A system that allows users to ask query about SQL and then get back SQL code that they can use for their application, think of it as a super basic version of the ChatGPT from OpenAI that I am sure everyone is quite familiar with.\n",
        "\n",
        "![ChatGPT OpenAI Example](https://drive.google.com/uc?id=1AGVZoxtWvBF6KLX0mkvpnU7mMEgaSraa)\n",
        "\n",
        "In the Week 1 Project we will be solidfying what we learnt throughout the week by building out this core functionality for the SQL Code Generation part of our application. Just like all Data Science projects we start off in a notebook environment to prototype things before having to bring them to production.\n",
        "\n",
        "Just like most LLM startups we will be leveraging OpenAI and their ChatGPT API initially for our product, in particular we will be using **gpt-4o-mini** which offers a good enough LLM that is quite fast!\n",
        "\n",
        "The main parts that we will be covering in the project are:\n",
        "1. [Using LLMs to Generate SQL Code](#scrollTo=HcGIbmPiM4o3)\n",
        "2. [Evaluating LLMs on SQL Code Generation](#scrollTo=pWz1btwOM7zr)\n",
        "3. [Validating LLM Outputs using Guardrails](#scrollTo=rPmPTQssNDUn)\n",
        "4. [Extra Credit: Optional Tasks](#scrollTo=UL-XI8PiNU1z&line=1&uniqifier=1)\n",
        "\n",
        "<a href=\"https://colab.research.google.com/drive/10y3EFKr4S8TsiSp00y8_LTD-2UU8nCGS?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6lSc2PMKFPyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up OpenAI API Key"
      ],
      "metadata": {
        "id": "9MduGWtPIPTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"\n",
        "  padding: 10px;\n",
        "  border-radius: 5px;\n",
        "  background-color: #ffcccc;\n",
        "  border-left: 6px solid #ff0000;\n",
        "  margin-bottom: 20px;\">\n",
        "  \n",
        "  <strong>⚠️ Important Notice:</strong>\n",
        "  <p>Do not share or use this API Key outside of the context of the notebook exercises.</p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "q1v3j1VcIWFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uplimit has provisioned an OpenAI API Key for your projects. Please add this API Key to this assignment by clicking on the Security Key icon on the left hand tab of the Google Colab notebook and then add a new parameter value called `OPENAI_API_KEY`.\n",
        "\n",
        "\n",
        " Here you can provide the API key that you copied and this will not be part of your Google Colab account. You can also enable the toggle Notebook access - this will allow your notebook to have access to this API key.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1SfE-nNOQ3DfZpJN6mAnxuqVsMv-F1_Yp\" />\n",
        "\n",
        "**NOTE:** We are hardcoding an API key for [Guardrails.ai](https://www.guardrailsai.com/) which is used for downloading different types of LLM Guardrails, access is free regardless and we hardcode it as a convenience for all the students."
      ],
      "metadata": {
        "id": "RzMCOyXfIZCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the API Key has been setup, run the following code:"
      ],
      "metadata": {
        "id": "nMMB_lzHIdpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai guardrails-ai==0.5.0 deepeval langchain langchain-openai"
      ],
      "metadata": {
        "id": "vcOQFQC7jUU9",
        "outputId": "3228925d-7980-4cf5-e335-b5cf04ee381f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Requirement already satisfied: guardrails-ai==0.5.0 in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: deepeval in /usr/local/lib/python3.10/dist-packages (1.5.6)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.17)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.1.25)\n",
            "Requirement already satisfied: coloredlogs<16.0.0,>=15.0.1 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (15.0.1)\n",
            "Requirement already satisfied: faker<26.0.0,>=25.2.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (25.9.2)\n",
            "Requirement already satisfied: griffe<0.37.0,>=0.36.9 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (0.36.9)\n",
            "Requirement already satisfied: guardrails-api-client>=0.3.8 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (0.3.13)\n",
            "Requirement already satisfied: jsonref<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (1.1.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (4.23.0)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.1 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (0.2.43)\n",
            "Requirement already satisfied: litellm<2.0.0,>=1.37.14 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (1.52.9)\n",
            "Requirement already satisfied: lxml<5.0.0,>=4.9.3 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (4.9.4)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (3.9.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (1.24.0)\n",
            "Requirement already satisfied: pip>=22 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (24.1.2)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (2.9.2)\n",
            "Requirement already satisfied: pydash<8.0.0,>=7.0.6 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (7.0.7)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (2.9.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (2.8.2)\n",
            "Requirement already satisfied: regex<2024.0.0,>=2023.10.3 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (2023.12.25)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.6.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (13.9.4)\n",
            "Requirement already satisfied: rstr<4.0.0,>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (3.2.2)\n",
            "Requirement already satisfied: tenacity>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (8.4.2)\n",
            "Requirement already satisfied: tiktoken>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (0.8.0)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<0.10.0,>=0.9.0->guardrails-ai==0.5.0) (0.9.4)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from deepeval) (8.3.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from deepeval) (0.9.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from deepeval) (4.25.5)\n",
            "Requirement already satisfied: sentry-sdk in /usr/local/lib/python3.10/dist-packages (from deepeval) (2.18.0)\n",
            "Requirement already satisfied: pytest-repeat in /usr/local/lib/python3.10/dist-packages (from deepeval) (0.9.3)\n",
            "Requirement already satisfied: pytest-xdist in /usr/local/lib/python3.10/dist-packages (from deepeval) (3.6.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from deepeval) (3.0.0)\n",
            "Requirement already satisfied: ragas in /usr/local/lib/python3.10/dist-packages (from deepeval) (0.2.5)\n",
            "Requirement already satisfied: docx2txt~=0.8 in /usr/local/lib/python3.10/dist-packages (from deepeval) (0.8)\n",
            "Requirement already satisfied: importlib-metadata>=6.0.2 in /usr/local/lib/python3.10/dist-packages (from deepeval) (7.0.0)\n",
            "Requirement already satisfied: opentelemetry-api~=1.24.0 in /usr/local/lib/python3.10/dist-packages (from deepeval) (1.24.0)\n",
            "Requirement already satisfied: grpcio~=1.63.0 in /usr/local/lib/python3.10/dist-packages (from deepeval) (1.63.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.4)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.142)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs<16.0.0,>=15.0.1->guardrails-ai==0.5.0) (10.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.10/dist-packages (from griffe<0.37.0,>=0.36.9->guardrails-ai==0.5.0) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-api-client>=0.3.8->guardrails-ai==0.5.0) (75.1.0)\n",
            "Requirement already satisfied: urllib3<2.1.0,>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from guardrails-api-client>=0.3.8->guardrails-ai==0.5.0) (2.0.7)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.0.2->deepeval) (3.21.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (0.21.0)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.10/dist-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.10/dist-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (3.0.0)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.10/dist-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (0.1.4)\n",
            "Requirement already satisfied: rfc3987 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (1.3.8)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.10/dist-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (24.11.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1->guardrails-ai==0.5.0) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1->guardrails-ai==0.5.0) (24.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai==0.5.0) (8.1.7)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai==0.5.0) (3.1.4)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai==0.5.0) (1.0.1)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai==0.5.0) (0.20.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->guardrails-ai==0.5.0) (1.4.2)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api~=1.24.0->deepeval) (1.2.14)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai==0.5.0) (1.66.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.24.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai==0.5.0) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.24.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai==0.5.0) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.45b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk<2.0.0,>=1.24.0->guardrails-ai==0.5.0) (0.45b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0,>=2.0.0->guardrails-ai==0.5.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0,>=2.0.0->guardrails-ai==0.5.0) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.8.2->guardrails-ai==0.5.0) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->guardrails-ai==0.5.0) (3.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.6.0->guardrails-ai==0.5.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.6.0->guardrails-ai==0.5.0) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<0.10.0,>=0.9.0->guardrails-ai==0.5.0) (1.5.4)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->deepeval) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest->deepeval) (1.5.0)\n",
            "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest->deepeval) (2.1.0)\n",
            "Requirement already satisfied: execnet>=2.1 in /usr/local/lib/python3.10/dist-packages (from pytest-xdist->deepeval) (2.1.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval) (3.1.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval) (0.2.19)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval) (1.6.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval) (1.4.4)\n",
            "Requirement already satisfied: pysbd>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval) (0.3.4)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api~=1.24.0->deepeval) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm<2.0.0,>=1.37.14->guardrails-ai==0.5.0) (3.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.6.0->guardrails-ai==0.5.0) (0.1.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->ragas->deepeval) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (0.26.2)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from isoduration->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (1.3.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community->ragas->deepeval) (0.6.7)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (2.9.0.20241003)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas->deepeval) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas->deepeval) (0.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas->deepeval) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas->deepeval) (2024.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas->deepeval) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"GUARDRAILS_TOKEN\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDEwNzI3MDE2NDYwNDM4MTg2OTU4OSIsImFwaUtleUlkIjoiYjAzNDVkNGEtZDhjNy00OThmLWIwZGYtZWI3ZTY0MzMwNTJkIiwiaWF0IjoxNzMwMTQ2ODc0LCJleHAiOjE3Mzc5MjI4NzR9.1J84O3pT_KlWTkGjV4zfBMChqYow868A-XTsePwiZ_Q\""
      ],
      "metadata": {
        "id": "OFBxDLxcIViM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Using LLMs to Generate SQL Code\n",
        "\n",
        "## SQL Code Generation\n",
        "\n",
        "SQL is one of the most powerful programming languages out there and is the main way in which we communicate with databases. Without it backend engineers would be unable to store and structure data to be easily queried to return to a frontend application and data scientists would be unable to track key metrics about their experiments over time.\n",
        "\n",
        "SQL is the language used for interacting with databases, while SQL is a specific database system that understands and uses SQL for database operations. Each database system, including SQL, implements SQL with some variations and adds its own proprietary extensions to the standard SQL language.\n",
        "\n",
        "Here is an example SQL statement:\n",
        "\n",
        "```sql\n",
        "SELECT\n",
        "    u.name,\n",
        "    u.email,\n",
        "    COUNT(o.order_id) AS total_orders,\n",
        "    SUM(o.amount) AS total_amount_spent\n",
        "FROM\n",
        "    users u\n",
        "JOIN\n",
        "    orders o ON u.user_id = o.user_id\n",
        "WHERE\n",
        "    o.order_date >= (NOW() - INTERVAL '1 year')\n",
        "GROUP BY\n",
        "    u.user_id\n",
        "HAVING\n",
        "    COUNT(o.order_id) > 5\n",
        "ORDER BY\n",
        "    total_amount_spent DESC;\n",
        "```\n",
        "\n",
        "LLM's such as ChatGPT from OpenAI are trained on all kinds of data across the internet and in particular they are trained on code across many different programming languages.\n",
        "\n",
        "\n",
        "![Text2SQL](https://drive.google.com/uc?id=17IDU11L_JdPIfMWvDa6li_5oOyGgWriX)\n",
        "\n",
        "What we will be performing is asking the LLM a Question related to a SQL query in order to create SQL code that we can then run.\n",
        "\n",
        "\n",
        "During OpenAI DevDay, there was a fantastic breakout section about **\"[A Survey of Techniques for Maximizing LLM Performance](https://youtu.be/ahnGLM-RC1Y?si=rifoSUxFgvgliFtT)\"** which we recommend all of you go through. In that talk, the speakers highlighted how we should always be starting off with Prompt Engineering when building LLM applications. Once we understand what our LLM application lacks can we only try more complex techniques such as RAG or Fine-tuning or both. With that spirit in mind let's try out some basic Prompt Engineering using ChatGPT.\n",
        "\n",
        "![Optimization Flow from OpenAI](https://drive.google.com/uc?id=1kcguLT8KYBmDypGB0fVyeT8YfrCiiMx_)"
      ],
      "metadata": {
        "id": "HcGIbmPiM4o3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import LLMResult, HumanMessage, Generation\n",
        "\n",
        "MODEL_NAME = \"gpt-4o-mini\"\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model_name=MODEL_NAME,\n",
        "    temperature=0.0\n",
        ")"
      ],
      "metadata": {
        "id": "82LGwE57Zc4a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Create a Baseline Zero-Shot Prompt\n",
        "\n",
        "Create a Zero-Shot Prompt that we can use in this situation, we have provided you with a simple prompt to try out. A **Zero-Shot prompt** is where we are asking the model to perform a task it has never seen explicty during training without any additional examples.\n",
        "\n",
        "When working on the prompt or improving the prompt remember a couple of things:\n",
        "\n",
        "1. Make sure to use clear instructions\n",
        "2. Try splitting tasks into simpler subtasks\n",
        "3. Give the model time to “think”\n",
        "\n",
        "![How to Prompt Engineer](https://drive.google.com/uc?id=1iSZ6XvD5eMiRSQBUT0DFEDA-1qc0qibv)\n"
      ],
      "metadata": {
        "id": "RE5HXufjHF8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Add your prompt here, make sure to use\n",
        "# {query} to inject a query into the prompt.\n",
        "\n",
        "ZERO_SHOT_PROMPT_TEMPLATE = \"\"\"\n",
        "You are an SQL exprt who can help with translating a query in natural language to SQL.\n",
        "Here is the query: {query}\n",
        "\"\"\"\n",
        "##You will be given a prompt which you will have to translate to SQL.\n",
        "\n",
        "\n",
        "# Try your own query\n",
        "SAMPLE_QUERY = \"Select the name of the employee who has the highest salary in each department\"\n",
        "\n",
        "prompt = ZERO_SHOT_PROMPT_TEMPLATE.format(\n",
        "    query=SAMPLE_QUERY\n",
        ")\n",
        "\n",
        "result = llm.generate([[HumanMessage(content=prompt)]])\n",
        "\n",
        "# # Do try to investigate the generations from LangChain to understand what is generated\n",
        "print(result.generations[0][0].text)"
      ],
      "metadata": {
        "id": "xcLCb77HbZ1B",
        "outputId": "ac1be92e-c473-48e3-ea9b-969304c30094",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To translate the query \"Select the name of the employee who has the highest salary in each department\" into SQL, you can use a common table expression (CTE) or a subquery along with the `GROUP BY` and `JOIN` clauses. Here’s one way to write the SQL query:\n",
            "\n",
            "```sql\n",
            "SELECT e.name\n",
            "FROM employees e\n",
            "JOIN (\n",
            "    SELECT department_id, MAX(salary) AS max_salary\n",
            "    FROM employees\n",
            "    GROUP BY department_id\n",
            ") AS max_salaries\n",
            "ON e.department_id = max_salaries.department_id AND e.salary = max_salaries.max_salary;\n",
            "```\n",
            "\n",
            "In this query:\n",
            "- We first create a subquery that selects the `department_id` and the maximum salary (`MAX(salary)`) for each department, grouping the results by `department_id`.\n",
            "- We then join this subquery back to the `employees` table to get the names of the employees who have the maximum salary in their respective departments.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great job! We have generated our very first SQL code using an LLM!\n",
        "\n",
        "An improvement we can perform on this is using a **Few-Shot prompt** where we include a couple of examples for the LLM to understand what we are trying to do. Doing so should result in better results as the LLM has some reference examples about what we are trying to do.\n",
        "\n",
        "**BUT** remember *we need to evaluate our LLM* prompt before we can even think about experimenting on it otherwise how would we even know if this new prompt is even better in the first place?"
      ],
      "metadata": {
        "id": "ys1WKl5JdIoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Evaluating LLMs on SQL Code Generation\n",
        "\n",
        "Evaluating Text-to-SQL might seem like an easy tasks to evaluate as all we need in order to generate our Golden Dataset would be a list of natural language queries and their respective SQL query but there are some intricacies with it that will prove to be challenging.\n",
        "\n",
        "We will need some way to check for **equivalence** to the **gold** query, which is where prior mentioned intricacies will start to pop up, but we will circle back to them later on. With that, we can start off with looking for an exact match between the LLM response and the golden query. For our initial metric we will be using a very simple metric, [exact match](https://huggingface.co/spaces/evaluate-metric/exact_match).\n",
        "\n",
        "![Basic Exact Match LLM Evaluator](https://drive.google.com/uc?id=1yIygL2zNC4JZ02dMBtC5HyNa0_ujIOOv)\n",
        "\n",
        "With a Golden Dataset which we can hand generate using our domain expertise, we can build up an evaluation dataset to run on every iteration to improve our application. Using the exact match metric we can derive an accuracy score of our LLM-application that we can keep track of and use in order to compare the performance across different iterations overtime.\n",
        "\n",
        "Here is an example of what our dataset can look like:\n",
        "\n",
        "![Example Golden Dataset](https://drive.google.com/uc?id=12D8C2juErtOzvKVLaKL_njxlceFJcbTu)\n",
        "\n",
        "**NOTE:** If you noticed, there is a **complexity** column on the SQL code used in the example which we can try incorporating as an optional tasks in order to improve the quality of our results across different fine-grained categories which is paramount in ensuring our LLM SQL Code Generator is more robust. Ideally we should have an even distribution across all these additional categories that we add on, where we can then calculate metrics such as **Accuracy@HardComplexity** and so on.\n",
        "\n",
        "We will provide you with a basic evaluation dataset to start off with, which we can further improve on using the process laid out below using [deepeval](https://github.com/confident-ai/deepeval):\n",
        "\n",
        "![Evaluation Process](https://drive.google.com/uc?id=1KC_gMvU3PeflEm0N1Co8_YnT46lCvMLU)\n",
        "<!-- 1. Load a Golden Test set for the students in a JSONL file with 20-30 examples\n",
        "2. TODO: Create a custom deep-eval evaluator for SQL code where we check if the output of the SQL code exactly matches the output from the LLM.\n",
        "3. TODO: Run evaluation on the golden test set.\n",
        "4. TODO: Try different prompts and see if they improve the evaluation metric\n",
        "5. Checking for an exact match is not the best way to go about it as the same query could have multiple ways to do it. Let's try using G-Eval where we use another more powerful LLM.\n",
        "6. TODO: Evaluate using G-Eval\n",
        "7. Talk about how in a Production system we should be running evaluation just like how we run automated tests, via CI/CD pipelines. -->"
      ],
      "metadata": {
        "id": "pWz1btwOM7zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASIC_GROUND_TRUTH_DATASET = [\n",
        "  {\n",
        "    \"Query\": \"Show the total number of employees.\",\n",
        "    \"Ground Truth\": \"SELECT COUNT(*) AS total_employees FROM employees;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Find the highest salary in the Finance department.\",\n",
        "    \"Ground Truth\": \"SELECT MAX(salary) FROM employees WHERE department = 'Finance';\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Get the average age of all managers.\",\n",
        "    \"Ground Truth\": \"SELECT AVG(age) FROM employees WHERE position = 'Manager';\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"List the names and emails of staff in the IT department.\",\n",
        "    \"Ground Truth\": \"SELECT name, email FROM employees WHERE department = 'IT';\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"What are the titles of the top 5 selling books?\",\n",
        "    \"Ground Truth\": \"SELECT title FROM books ORDER BY sales DESC LIMIT 5;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Which product has the least quantity in stock?\",\n",
        "    \"Ground Truth\": \"SELECT product_name FROM products ORDER BY quantity_in_stock ASC LIMIT 1;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Display the second highest salary in the organization.\",\n",
        "    \"Ground Truth\": \"SELECT MAX(salary) FROM employees WHERE salary < (SELECT MAX(salary) FROM employees);\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"List employees who joined after 2015 and work in the Sales department.\",\n",
        "    \"Ground Truth\": \"SELECT * FROM employees WHERE year(joined_date) > 2015 AND department = 'Sales';\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Find the average order value for each customer.\",\n",
        "    \"Ground Truth\": \"SELECT customer_id, AVG(order_value) FROM orders GROUP BY customer_id;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Show departments that have more than 10 employees.\",\n",
        "    \"Ground Truth\": \"SELECT department FROM employees GROUP BY department HAVING COUNT(*) > 10;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"List all products that have never been ordered.\",\n",
        "    \"Ground Truth\": \"SELECT * FROM products WHERE product_id NOT IN (SELECT product_id FROM orders);\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Which customers have spent more than $1000 in total?\",\n",
        "    \"Ground Truth\": \"SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(order_value) > 1000;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Show the total number of orders placed each day last week.\",\n",
        "    \"Ground Truth\": \"SELECT order_date, COUNT(*) FROM orders WHERE order_date > CURRENT_DATE - INTERVAL '7 days' GROUP BY order_date;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"List the names of employees who do not manage anyone.\",\n",
        "    \"Ground Truth\": \"SELECT name FROM employees WHERE name NOT IN (SELECT manager_name FROM employees);\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Display the department that has the highest average employee salary.\",\n",
        "    \"Ground Truth\": \"SELECT department FROM employees GROUP BY department ORDER BY AVG(salary) DESC LIMIT 1;\"\n",
        "  }\n",
        "]"
      ],
      "metadata": {
        "id": "dKEiy2NmToCC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(BASIC_GROUND_TRUTH_DATASET)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JSrkIWZETx2W",
        "outputId": "05169b47-8d37-47b8-ef8e-846e6627496d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Query  \\\n",
              "0                Show the total number of employees.   \n",
              "1  Find the highest salary in the Finance departm...   \n",
              "2               Get the average age of all managers.   \n",
              "3  List the names and emails of staff in the IT d...   \n",
              "4    What are the titles of the top 5 selling books?   \n",
              "\n",
              "                                        Ground Truth  \n",
              "0  SELECT COUNT(*) AS total_employees FROM employ...  \n",
              "1  SELECT MAX(salary) FROM employees WHERE depart...  \n",
              "2  SELECT AVG(age) FROM employees WHERE position ...  \n",
              "3  SELECT name, email FROM employees WHERE depart...  \n",
              "4  SELECT title FROM books ORDER BY sales DESC LI...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fc85410f-a6b1-421d-a014-7a3f3929873c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Query</th>\n",
              "      <th>Ground Truth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Show the total number of employees.</td>\n",
              "      <td>SELECT COUNT(*) AS total_employees FROM employ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Find the highest salary in the Finance departm...</td>\n",
              "      <td>SELECT MAX(salary) FROM employees WHERE depart...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Get the average age of all managers.</td>\n",
              "      <td>SELECT AVG(age) FROM employees WHERE position ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>List the names and emails of staff in the IT d...</td>\n",
              "      <td>SELECT name, email FROM employees WHERE depart...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What are the titles of the top 5 selling books?</td>\n",
              "      <td>SELECT title FROM books ORDER BY sales DESC LI...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc85410f-a6b1-421d-a014-7a3f3929873c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fc85410f-a6b1-421d-a014-7a3f3929873c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fc85410f-a6b1-421d-a014-7a3f3929873c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b897eb90-d58e-4b29-ad4c-a4bef6bc0809\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b897eb90-d58e-4b29-ad4c-a4bef6bc0809')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b897eb90-d58e-4b29-ad4c-a4bef6bc0809 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"Query\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"Show departments that have more than 10 employees.\",\n          \"Which customers have spent more than $1000 in total?\",\n          \"Show the total number of employees.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ground Truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"SELECT department FROM employees GROUP BY department HAVING COUNT(*) > 10;\",\n          \"SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(order_value) > 1000;\",\n          \"SELECT COUNT(*) AS total_employees FROM employees;\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next step, lets see how our LLM and Prompt are able to perform on the entire dataset by running it through and evaluating its performance. This is known as performing a LLM Task Evaluation which is different than LLM Model Evals. Task evaluation cares about the performance of the LLM on a specific task whereas Model evals care about the general performance of the LLM across a variety of tasks, benchmarks such as the [HellaSwag](https://rowanzellers.com/hellaswag/) dataset is an example of a Model Evals and Text2SQL accuracy like we are doing in this project is an example of task evaluation.\n",
        "\n",
        "We will start off with traditional NLP metrics that compare the output with the ground truth can derive a score based on the exact/similarity of the two. These metrics are simple to use and serve as a good place to start off when evaluating LLMs.\n",
        "\n",
        "Firstly, let's start off with the exact match metric which very simply just compares the output with the ground truth and checks whether its an exact match or not, deepeval implements these metrics within the [Scorer module](https://github.com/confident-ai/deepeval/blob/4b3ceed20993232331550798fe0a8f1bf2605594/deepeval/scorer/scorer.py#L7).\n",
        "\n",
        "deepeval makes it easy to create our own metrics by inheriting the BaseMetric class, you can read more about creating custom metrics [here](https://docs.confident-ai.com/docs/metrics-custom)."
      ],
      "metadata": {
        "id": "qBOyfpn54-1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import BaseMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.scorer import Scorer\n",
        "\n",
        "\n",
        "class ExactMatchMetric(BaseMetric):\n",
        "    def __init__(self, threshold: float = 0.0, async_mode: bool = True):\n",
        "        self.threshold = threshold\n",
        "        self.async_mode = async_mode\n",
        "\n",
        "    def _base_measure(self, test_case: LLMTestCase):\n",
        "        self.success = Scorer.exact_match_score(\n",
        "            test_case.actual_output, test_case.expected_output\n",
        "        )\n",
        "        if self.success:\n",
        "            self.score = 1\n",
        "        else:\n",
        "            self.score = 0\n",
        "        return self.score\n",
        "\n",
        "    def measure(self, test_case: LLMTestCase):\n",
        "        self._base_measure(test_case=test_case)\n",
        "\n",
        "    async def a_measure(self, test_case: LLMTestCase):\n",
        "        self._base_measure(test_case=test_case)\n",
        "\n",
        "    def is_successful(self):\n",
        "        return bool(self.success)\n",
        "\n",
        "    @property\n",
        "    def __name__(self):\n",
        "        return \"ExactMatch\""
      ],
      "metadata": {
        "id": "PPOcKnpcUlD0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create our first test case!\n",
        "\n",
        "test_case_input = df.head(1)\n",
        "test_case_query = test_case_input['Query'].squeeze()\n",
        "test_case_expected_output = test_case_input['Ground Truth'].squeeze()"
      ],
      "metadata": {
        "id": "5apnNHGFYvno"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Here is our query: {test_case_query}\")\n",
        "print(f\"Here is our ground truth: {test_case_expected_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrLlDUKMb7oq",
        "outputId": "7aff40ed-e0de-4d13-9cc2-1500e0e4b9fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is our query: Show the total number of employees.\n",
            "Here is our ground truth: SELECT COUNT(*) AS total_employees FROM employees;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ZERO_SHOT_PROMPT_TEMPLATE = \"\"\"\n",
        "Generate a valid SQL query for the following natural language instruction:\n",
        "\n",
        "${query}\n",
        "\n",
        "Only generate SQL code and nothing else.\n",
        "\"\"\"\n",
        "\n",
        "ZERO_SHOT_PROMPT_TEMPLATE = \"\"\"\n",
        "You are an SQL exprt who can help with translating a query in natural language to SQL.\n",
        "Here is the query: {query}\n",
        "Only generate SQL code and nothing else.\n",
        "\n",
        "\"\"\"\n",
        "prompt = ZERO_SHOT_PROMPT_TEMPLATE.format(\n",
        "    query=test_case_query\n",
        ")\n",
        "\n",
        "result = llm.generate([[HumanMessage(content=prompt)]])\n",
        "\n",
        "test_case_actual_output = result.generations[0][0].text"
      ],
      "metadata": {
        "id": "7twvbNfOaMaW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_case_expected_output"
      ],
      "metadata": {
        "id": "MnnLy2Ho-A68",
        "outputId": "a7e3cf72-6a8a-4be0-81ac-679434405e91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'SELECT COUNT(*) AS total_employees FROM employees;'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Here is what our model generated: {test_case_actual_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue1bMG-ubEpt",
        "outputId": "fa90084d-18b8-4f21-d847-bd99406e913b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is what our model generated: ```sql\n",
            "SELECT COUNT(*) AS total_employees FROM employees;\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "  input=test_case_input,\n",
        "  actual_output= \"SELECT COUNT(*) AS total_employees FROM employees;\",\n",
        "  expected_output=test_case_expected_output\n",
        ")"
      ],
      "metadata": {
        "id": "9FQdZAxXZrQ3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "exact_match_metric = ExactMatchMetric()\n",
        "exact_match_metric.measure(test_case)"
      ],
      "metadata": {
        "id": "9oQx25b7ctYh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test case: {exact_match_metric.is_successful()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM2s34m-dBKW",
        "outputId": "723c5a22-7adb-43ae-c58a-a864a09e9adc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test case: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** If your prompt still fails and produces a different SQL code such as *\"SELECT COUNT(*) AS total_employees FROM employees;\"* feel free to update the ground truth to get it to pass in the meantime. This can happen as code correctness is not a simple task due to the fact that different implementations can still solve the query which is something we invite you to address via the optional tasks for this week.\n",
        "\n",
        "\n",
        "Great job! We are able to see that our LLM and Prompt are able to answer the test case correctly, but that is just a basic example that we have. Next let us run it on the entire dataset that we have created.\n",
        "\n",
        "## TODO: Create a deepeval dataset and evalute our LLM and Prompt on it\n",
        "\n",
        "There is a slight difference between what we have been learning about Golden Datasets in general and how Golden Datasets work within deepeval. When we talk about a Golden Dataset as per the [course materials here](https://uplimit.com/course/llms-in-production/v2/module/evaluating-llms#corise_clrnjsvpd001m2e6iogarwdpn) we mention that its similar to a held-out test set meaning that we have data to predict on alongside their ground truth labels in order to derive evaluation metrics.\n",
        "\n",
        "In deepeval, we will need to create an Evaluation Dataset instead where an Evaluation Dataset is built from LLMTestCase's and/or Golden's. Remember that the `actual_output` comes from running our LLM against the `input` as we did above!\n",
        "```python\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "\n",
        "first_test_case = LLMTestCase(input=\"...\", actual_output=\"...\")\n",
        "second_test_case = LLMTestCase(input=\"...\", actual_output=\"...\")\n",
        "\n",
        "test_cases = [first_test_case, second_test_case]\n",
        "dataset = EvaluationDataset(test_cases=test_cases)\n",
        "```\n",
        "The definition of Golden's within deepeval is documented [here](https://docs.confident-ai.com/docs/confident-ai-manage-datasets#what-is-a-golden):\n",
        "```\n",
        "A \"Golden\" is what makes up an evaluation dataset and is very similar to a test case in deepeval, but they:\n",
        "\n",
        "    - do not require an actual_output, so whilst test cases are always ready for evaluation, a golden isn't.\n",
        "    - only exists within an EvaluationDataset(), while test cases can be defined anywhere.\n",
        "    - contains an extra additional_metadata field, which is a dictionary you can define on Confident. Allows you to do some extra preprocessing on your dataset (eg., generating a custom LLM actual_output based on some variables in additional_metadata) before evaluation.\n",
        "\n",
        "We introduced the concept of goldens because it allows you to create evaluation datasets on Confident without needing pre-computed actual_outputs. This is especially helpful if you are looking to generate responses from your LLM application at evaluation time.\n",
        "```\n",
        "With our EvaluationDataset created we can then run our LLM through it in order to derive the final score. For example, the following does so without using Pytest although its recommended that you treat this process as a CI/CD process that runs alongside your Pytest suite for your actual application.\n",
        "\n",
        "```python\n",
        "from deepeval import evaluate\n",
        "from deepeval.metrics import HallucinationMetric, AnswerRelevancyMetric\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "\n",
        "dataset = EvaluationDataset(test_cases=[...])\n",
        "hallucination_metric = HallucinationMetric(threshold=0.3)\n",
        "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
        "\n",
        "dataset.evaluate([hallucination_metric, answer_relevancy_metric])\n",
        "\n",
        "# You can also call the evaluate() function directly\n",
        "evaluate(dataset, [hallucination_metric, answer_relevancy_metric])\n",
        "```\n",
        "An example output of the evaluation would look like this:\n",
        "```\n",
        "======================================================================\n",
        "\n",
        "Metrics Summary\n",
        "\n",
        "  - ✅ Answer Relevancy (score: 1, threshold: 0.5, strict: False, evaluation model: gpt-4-turbo, reason: The score is 1.00 because the response perfectly addresses the question about operating hours without any irrelevant information. Great job!, error: None)\n",
        "  - ✅ Bias (score: 0, threshold: 0.5, strict: False, evaluation model: gpt-4-turbo, reason: The score is 0.00 because the output maintains neutrality and objectivity, as evidenced by the absence of any cited biased phrases or issues., error: None)\n",
        "\n",
        "For test case:\n",
        "\n",
        "  - input: What are your operating hours?\n",
        "  - actual output: ...\n",
        "  - expected output: None\n",
        "  - context: ['Our company operates from 10 AM to 6 PM, Monday to Friday.', 'We are closed on weekends and public holidays.', 'Our customer service is available 24/7.']\n",
        "  - retrieval context: None\n",
        "\n",
        "======================================================================\n",
        "\n",
        "Metrics Summary\n",
        "\n",
        "  - ✅ Answer Relevancy (score: 1, threshold: 0.5, strict: False, evaluation model: gpt-4-turbo, reason: The score is 1.00 because the response accurately addresses the question about free shipping without any irrelevant information. Great job on maintaining focus!, error: None)\n",
        "  - ✅ Bias (score: 0, threshold: 0.5, strict: False, evaluation model: gpt-4-turbo, reason: The score is 0.00 because the actual output perfectly demonstrates neutrality and objectivity, without any indication of bias., error: None)\n",
        "\n",
        "For test case:\n",
        "\n",
        "  - input: Do you offer free shipping?\n",
        "  - actual output: ...\n",
        "  - expected output: Yes, we offer free shipping on orders over $50.\n",
        "  - context: None\n",
        "  - retrieval context: None\n",
        "\n",
        "======================================================================\n",
        "```\n",
        "One thing to note is that the metrics are run on a single (query, ground_truth) instead across the entire dataset meaning that we lack the ability to have a singular number to compare across different evaluation runs like traditional machine learning where you would for example have a accuracy score for the test set.\n",
        "\n",
        "Let us introduce the concept of pass rates which are similar to accuracy across the entire dataset but its just meant to show how many instances for a metric in the dataset passed a evaluation metric check.\n",
        "\n",
        "With this you could enforce a strict pass rate on deployments much like how we can enforce Pytest code coverage to be off a certain %.\n",
        "\n",
        "deepeval supports this via the `aggregate_metric_pass_rates()` on the results of the evaluation give us pass rates across each metric that we use:\n",
        "\n",
        "```python\n",
        "from deepeval.evaluate import aggregate_metric_pass_rates\n",
        "\n",
        "evaluation_results = evaluate(dataset, [hallucination_metric, answer_relevancy_metric])\n",
        "aggregate_metric_pass_rates(evaluation_results.test_results)\n",
        "```\n",
        "\n",
        "This should output the following where we get the pass rates across each metric and a mapping storing the pass rates across each metric which we can use to store and compare across different evaluation runs:\n",
        "```\n",
        "======================================================================\n",
        "\n",
        "Aggregate Metric Pass Rates\n",
        "\n",
        "AnswerRelevancyMetric: 100.00% pass rate\n",
        "BiasMetric: 100.00% pass rate\n",
        "\n",
        "======================================================================\n",
        "\n",
        "{'AnswerRelevancyMetric': 1.0, 'BiasMetric': 1.0}\n",
        "```\n",
        "\n",
        "Now with this you can apply it to our dataset:\n",
        "1. Create an [EvaluationDataset](https://docs.confident-ai.com/docs/evaluation-datasets#create-an-evaluation-dataset) using deepeval.\n",
        "2. Run the evaluation on the Golden Dataset without Pytest, refer to [this](https://docs.confident-ai.com/docs/evaluation-introduction#evaluating-without-pytest)."
      ],
      "metadata": {
        "id": "n-A23MUVfNhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################\n",
        "# START YOUR CODE HERE\n",
        "######################\n",
        "\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "\n",
        "def generate_sql_code(test_case_query: str) -> str:\n",
        "    ZERO_SHOT_PROMPT_TEMPLATE = \"\"\"\n",
        "    You are an expert in writing SQL queries. Your Task is to generate a valid SQL query for the following natural language instruction:\n",
        "    ${query}\n",
        "    You are striclty asked to only generate SQL code and nothing else.\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    Your Task is to generate a valid SQL query for the following natural language instruction:\n",
        "    ${query}\n",
        "    Only generate SQL code and nothing else.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    prompt = ZERO_SHOT_PROMPT_TEMPLATE.format(\n",
        "        query=test_case_query\n",
        "    )\n",
        "\n",
        "    result = llm.generate([[HumanMessage(content=prompt)]])\n",
        "    test_case_actual_output = result.generations[0][0].text.strip()\n",
        "    return test_case_actual_output\n",
        "\n",
        "test_cases = []\n",
        "for input_output_pair in BASIC_GROUND_TRUTH_DATASET:\n",
        "  test_case_actual_output = generate_sql_code(input_output_pair[\"Query\"])\n",
        "  test_case = LLMTestCase(\n",
        "      input = input_output_pair[\"Query\"],\n",
        "      actual_output=test_case_actual_output,\n",
        "      expected_output=input_output_pair[\"Ground Truth\"],\n",
        "      context=[\"\"],\n",
        "  )\n",
        "  test_cases.append(test_case)\n",
        "\n",
        "dataset = EvaluationDataset(test_cases=test_cases)"
      ],
      "metadata": {
        "id": "RJ_wARozNj82"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval import evaluate\n",
        "from deepeval.metrics import HallucinationMetric, AnswerRelevancyMetric\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "\n",
        "dataset = EvaluationDataset(test_cases=test_cases)\n",
        "hallucination_metric = HallucinationMetric(threshold=0.3)\n",
        "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
        "\n",
        "dataset.evaluate([hallucination_metric, answer_relevancy_metric])"
      ],
      "metadata": {
        "id": "LCzn0Ahqy4FL",
        "outputId": "703c47f6-2e7f-400d-9b19-f6791ca3f332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating 15 test case(s) in parallel: |██████████|100% (15/15) [Time Taken: 00:15,  1.00s/test case]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no contradictions and no context provided, making it impossible for the actual output to contradict any information., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response was perfectly relevant, providing exactly what was requested without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List the names and emails of staff in the IT department.\n",
            "  - actual output: ```sql\n",
            "SELECT name, email FROM staff WHERE department = 'IT';\n",
            "```\n",
            "  - expected output: SELECT name, email FROM employees WHERE department = 'IT';\n",
            "  - context: ['']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output does not contradict the provided context, which is empty and thus provides no information to conflict with., error: None)\n",
            "  - ❌ Answer Relevancy (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the output consists entirely of SQL commands, which are irrelevant as they do not directly answer the question about the titles of the top 5 selling books., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: What are the titles of the top 5 selling books?\n",
            "  - actual output: ```sql\n",
            "SELECT title \n",
            "FROM books \n",
            "ORDER BY sales DESC \n",
            "LIMIT 5;\n",
            "```\n",
            "  - expected output: SELECT title FROM books ORDER BY sales DESC LIMIT 5;\n",
            "  - context: ['']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the context is empty, so there is no information to contradict the actual output., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addresses the request without any irrelevant information. Great job on staying focused and providing exactly what was needed!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Display the second highest salary in the organization.\n",
            "  - actual output: ```sql\n",
            "SELECT MAX(salary) AS SecondHighestSalary\n",
            "FROM employees\n",
            "WHERE salary < (SELECT MAX(salary) FROM employees);\n",
            "```\n",
            "  - expected output: SELECT MAX(salary) FROM employees WHERE salary < (SELECT MAX(salary) FROM employees);\n",
            "  - context: ['']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the context is empty, and there are no contradictions between the actual output and provided information., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the input request with no irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Display the department that has the highest average employee salary.\n",
            "  - actual output: ```sql\n",
            "SELECT department, AVG(salary) AS average_salary\n",
            "FROM employees\n",
            "GROUP BY department\n",
            "ORDER BY average_salary DESC\n",
            "LIMIT 1;\n",
            "```\n",
            "  - expected output: SELECT department FROM employees GROUP BY department ORDER BY AVG(salary) DESC LIMIT 1;\n",
            "  - context: ['']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there is no context provided, and thus, no contradictions can occur with the actual output., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addresses the input without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of employees.\n",
            "  - actual output: ```sql\n",
            "SELECT COUNT(*) AS total_employees FROM employees;\n",
            "```\n",
            "  - expected output: SELECT COUNT(*) AS total_employees FROM employees;\n",
            "  - context: ['']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there is no context provided, so the actual output cannot contradict any given information., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is perfectly relevant and directly addresses the input without any irrelevant statements. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List employees who joined after 2015 and work in the Sales department.\n",
            "  - actual output: ```sql\n",
            "SELECT * FROM employees \n",
            "WHERE join_date > '2015-01-01' \n",
            "AND department = 'Sales';\n",
            "```\n",
            "  - expected output: SELECT * FROM employees WHERE year(joined_date) > 2015 AND department = 'Sales';\n",
            "  - context: ['']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the context is empty, so there is no information to contradict the actual output., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is perfectly relevant, addressing the request without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Get the average age of all managers.\n",
            "  - actual output: ```sql\n",
            "SELECT AVG(age) AS average_age\n",
            "FROM employees\n",
            "WHERE position = 'manager';\n",
            "```\n",
            "  - expected output: SELECT AVG(age) FROM employees WHERE position = 'Manager';\n",
            "  - context: ['']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no contradictions between the actual output and the contexts, indicating perfect factual alignment., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addresses the input without any irrelevant statements. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List all products that have never been ordered.\n",
            "  - actual output: ```sql\n",
            "SELECT p.*\n",
            "FROM products p\n",
            "LEFT JOIN orders o ON p.product_id = o.product_id\n",
            "WHERE o.order_id IS NULL;\n",
            "```\n",
            "  - expected output: SELECT * FROM products WHERE product_id NOT IN (SELECT product_id FROM orders);\n",
            "  - context: ['']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the context is empty, so there is nothing to contradict the actual output., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the request without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of orders placed each day last week.\n",
            "  - actual output: ```sql\n",
            "SELECT DATE(order_date) AS order_day, COUNT(*) AS total_orders\n",
            "FROM orders\n",
            "WHERE order_date >= CURDATE() - INTERVAL 7 DAY AND order_date < CURDATE()\n",
            "GROUP BY order_day\n",
            "ORDER BY order_day;\n",
            "```\n",
            "  - expected output: SELECT order_date, COUNT(*) FROM orders WHERE order_date > CURRENT_DATE - INTERVAL '7 days' GROUP BY order_date;\n",
            "  - context: ['']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the context is empty, so there is no information to contradict the actual output., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addressed the request without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the average order value for each customer.\n",
            "  - actual output: ```sql\n",
            "SELECT customer_id, AVG(order_value) AS average_order_value\n",
            "FROM orders\n",
            "GROUP BY customer_id;\n",
            "```\n",
            "  - expected output: SELECT customer_id, AVG(order_value) FROM orders GROUP BY customer_id;\n",
            "  - context: ['']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the context is empty, so the actual output cannot contradict any information., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addressed the input without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List the names of employees who do not manage anyone.\n",
            "  - actual output: ```sql\n",
            "SELECT e.name\n",
            "FROM employees e\n",
            "LEFT JOIN employees m ON e.id = m.manager_id\n",
            "WHERE m.id IS NULL;\n",
            "```\n",
            "  - expected output: SELECT name FROM employees WHERE name NOT IN (SELECT manager_name FROM employees);\n",
            "  - context: ['']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output does not contradict the empty context, indicating no hallucination as there is no information provided to evaluate against., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is perfectly relevant and directly addresses the question, providing exactly what was asked without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Which customers have spent more than $1000 in total?\n",
            "  - actual output: ```sql\n",
            "SELECT customer_id\n",
            "FROM orders\n",
            "GROUP BY customer_id\n",
            "HAVING SUM(total_amount) > 1000;\n",
            "```\n",
            "  - expected output: SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(order_value) > 1000;\n",
            "  - context: ['']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no contradictions between the actual output and the context, indicating no hallucination., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is perfectly relevant, addressing the request directly without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the highest salary in the Finance department.\n",
            "  - actual output: ```sql\n",
            "SELECT MAX(salary) \n",
            "FROM employees \n",
            "WHERE department = 'Finance';\n",
            "```\n",
            "  - expected output: SELECT MAX(salary) FROM employees WHERE department = 'Finance';\n",
            "  - context: ['']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no contradictions found, and the actual output is considered to align with the context by default due to the absence of any conflicting information., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addressed the input without any irrelevant statements. Great job on staying focused and relevant!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show departments that have more than 10 employees.\n",
            "  - actual output: ```sql\n",
            "SELECT department_id, COUNT(employee_id) AS employee_count\n",
            "FROM employees\n",
            "GROUP BY department_id\n",
            "HAVING COUNT(employee_id) > 10;\n",
            "```\n",
            "  - expected output: SELECT department FROM employees GROUP BY department HAVING COUNT(*) > 10;\n",
            "  - context: ['']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there is no context provided, eliminating any possibility of contradiction with the actual output., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the actual output is perfectly relevant and directly addresses the question without any irrelevant statements. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Which product has the least quantity in stock?\n",
            "  - actual output: ```sql\n",
            "SELECT product_name\n",
            "FROM products\n",
            "ORDER BY quantity_in_stock ASC\n",
            "LIMIT 1;\n",
            "```\n",
            "  - expected output: SELECT product_name FROM products ORDER BY quantity_in_stock ASC LIMIT 1;\n",
            "  - context: ['']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Hallucination: 100.00% pass rate\n",
            "Answer Relevancy: 93.33% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EvaluationResult(test_results=[TestResult(name='test_case_3', success=True, metrics_data=[MetricData(name='Hallucination', threshold=0.3, success=True, score=0.0, reason='The score is 0.00 because there are no contradictions and no context provided, making it impossible for the actual output to contradict any information.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0022775, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"There is no context provided, so the actual output cannot contradict any information.\"\\n    }\\n]'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the response was perfectly relevant, providing exactly what was requested without any irrelevant information. Great job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.00281, verbose_logs='Statements:\\n[\\n    \"SELECT name, email FROM staff WHERE department = \\'IT\\';\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='List the names and emails of staff in the IT department.', actual_output=\"```sql\\nSELECT name, email FROM staff WHERE department = 'IT';\\n```\", expected_output=\"SELECT name, email FROM employees WHERE department = 'IT';\", context=[''], retrieval_context=None), TestResult(name='test_case_4', success=False, metrics_data=[MetricData(name='Hallucination', threshold=0.3, success=True, score=0.0, reason='The score is 0.00 because the actual output does not contradict the provided context, which is empty and thus provides no information to conflict with.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0024200000000000003, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The actual output does not contradict the provided context, as the context is empty and does not provide any information to conflict with.\"\\n    }\\n]'), MetricData(name='Answer Relevancy', threshold=0.5, success=False, score=0.0, reason='The score is 0.00 because the output consists entirely of SQL commands, which are irrelevant as they do not directly answer the question about the titles of the top 5 selling books.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0048200000000000005, verbose_logs='Statements:\\n[\\n    \"SELECT title\",\\n    \"FROM books\",\\n    \"ORDER BY sales DESC\",\\n    \"LIMIT 5;\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The statement \\'SELECT title\\' is a SQL command and does not directly provide the titles of the top 5 selling books.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The statement \\'FROM books\\' is a SQL command and does not directly provide the titles of the top 5 selling books.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The statement \\'ORDER BY sales DESC\\' is a SQL command and does not directly provide the titles of the top 5 selling books.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The statement \\'LIMIT 5;\\' is a SQL command and does not directly provide the titles of the top 5 selling books.\"\\n    }\\n]')], conversational=False, multimodal=False, input='What are the titles of the top 5 selling books?', actual_output='```sql\\nSELECT title \\nFROM books \\nORDER BY sales DESC \\nLIMIT 5;\\n```', expected_output='SELECT title FROM books ORDER BY sales DESC LIMIT 5;', context=[''], retrieval_context=None), TestResult(name='test_case_6', success=True, metrics_data=[MetricData(name='Hallucination', threshold=0.3, success=True, score=0.0, reason='The score is 0.00 because the context is empty, so there is no information to contradict the actual output.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0022725, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context is empty, so there is no information to contradict the actual output.\"\\n    }\\n]'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the response perfectly addresses the request without any irrelevant information. Great job on staying focused and providing exactly what was needed!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0033049999999999998, verbose_logs='Statements:\\n[\\n    \"SELECT MAX(salary) AS SecondHighestSalary\",\\n    \"FROM employees\",\\n    \"WHERE salary < (SELECT MAX(salary) FROM employees)\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='Display the second highest salary in the organization.', actual_output='```sql\\nSELECT MAX(salary) AS SecondHighestSalary\\nFROM employees\\nWHERE salary < (SELECT MAX(salary) FROM employees);\\n```', expected_output='SELECT MAX(salary) FROM employees WHERE salary < (SELECT MAX(salary) FROM employees);', context=[''], retrieval_context=None), TestResult(name='test_case_14', success=True, metrics_data=[MetricData(name='Hallucination', threshold=0.3, success=True, score=0.0, reason='The score is 0.00 because the context is empty, and there are no contradictions between the actual output and provided information.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0023, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context is empty, so the actual output does not contradict any provided information.\"\\n    }\\n]'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the output perfectly addresses the input request with no irrelevant information. Great job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.002945, verbose_logs='Statements:\\n[\\n    \"SELECT department, AVG(salary) AS average_salary FROM employees GROUP BY department ORDER BY average_salary DESC LIMIT 1;\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='Display the department that has the highest average employee salary.', actual_output='```sql\\nSELECT department, AVG(salary) AS average_salary\\nFROM employees\\nGROUP BY department\\nORDER BY average_salary DESC\\nLIMIT 1;\\n```', expected_output='SELECT department FROM employees GROUP BY department ORDER BY AVG(salary) DESC LIMIT 1;', context=[''], retrieval_context=None), TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Hallucination', threshold=0.3, success=True, score=0.0, reason='The score is 0.00 because there is no context provided, and thus, no contradictions can occur with the actual output.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.002255, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"There is no context provided, so the actual output cannot contradict any given information.\"\\n    }\\n]'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the response perfectly addresses the input without any irrelevant information. Great job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0026650000000000003, verbose_logs='Statements:\\n[\\n    \"SELECT COUNT(*) AS total_employees FROM employees;\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='Show the total number of employees.', actual_output='```sql\\nSELECT COUNT(*) AS total_employees FROM employees;\\n```', expected_output='SELECT COUNT(*) AS total_employees FROM employees;', context=[''], retrieval_context=None), TestResult(name='test_case_7', success=True, metrics_data=[MetricData(name='Hallucination', threshold=0.3, success=True, score=0.0, reason='The score is 0.00 because there is no context provided, so the actual output cannot contradict any given information.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0022700000000000003, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"There is no context provided, so the actual output cannot contradict any given information.\"\\n    }\\n]'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the output is perfectly relevant and directly addresses the input without any irrelevant statements. Great job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0032774999999999996, verbose_logs='Statements:\\n[\\n    \"SELECT * FROM employees\",\\n    \"WHERE join_date > \\'2015-01-01\\'\",\\n    \"AND department = \\'Sales\\'\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='List employees who joined after 2015 and work in the Sales department.', actual_output=\"```sql\\nSELECT * FROM employees \\nWHERE join_date > '2015-01-01' \\nAND department = 'Sales';\\n```\", expected_output=\"SELECT * FROM employees WHERE year(joined_date) > 2015 AND department = 'Sales';\", context=[''], retrieval_context=None), TestResult(name='test_case_2', success=True, metrics_data=[MetricData(name='Hallucination', threshold=0.3, success=True, score=0.0, reason='The score is 0.00 because the context is empty, so there is no information to contradict the actual output.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0022525, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context is empty, so there is no information to contradict the actual output.\"\\n    }\\n]'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the response is perfectly relevant, addressing the request without any irrelevant information. Great job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.00312, verbose_logs='Statements:\\n[\\n    \"SELECT AVG(age) AS average_age\",\\n    \"FROM employees\",\\n    \"WHERE position = \\'manager\\'\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='Get the average age of all managers.', actual_output=\"```sql\\nSELECT AVG(age) AS average_age\\nFROM employees\\nWHERE position = 'manager';\\n```\", expected_output=\"SELECT AVG(age) FROM employees WHERE position = 'Manager';\", context=[''], retrieval_context=None), TestResult(name='test_case_10', success=True, metrics_data=[MetricData(name='Hallucination', threshold=0.3, success=True, score=0.0, reason='The score is 0.00 because there are no contradictions between the actual output and the contexts, indicating perfect factual alignment.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.002225, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"There is no context provided to contradict the actual output.\"\\n    }\\n]'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the response perfectly addresses the input without any irrelevant statements. Great job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0033975000000000003, verbose_logs='Statements:\\n[\\n    \"SELECT p.*\",\\n    \"FROM products p\",\\n    \"LEFT JOIN orders o ON p.product_id = o.product_id\",\\n    \"WHERE o.order_id IS NULL\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='List all products that have never been ordered.', actual_output='```sql\\nSELECT p.*\\nFROM products p\\nLEFT JOIN orders o ON p.product_id = o.product_id\\nWHERE o.order_id IS NULL;\\n```', expected_output='SELECT * FROM products WHERE product_id NOT IN (SELECT product_id FROM orders);', context=[''], retrieval_context=None), TestResult(name='test_case_12', success=True, metrics_data=[MetricData(name='Hallucination', threshold=0.3, success=True, score=0.0, reason='The score is 0.00 because the context is empty, so there is nothing to contradict the actual output.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0023074999999999997, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context is empty, so there is nothing to contradict the actual output.\"\\n    }\\n]'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the output perfectly addresses the request without any irrelevant information. Great job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.00387, verbose_logs='Statements:\\n[\\n    \"SELECT DATE(order_date) AS order_day, COUNT(*) AS total_orders\",\\n    \"FROM orders\",\\n    \"WHERE order_date >= CURDATE() - INTERVAL 7 DAY AND order_date < CURDATE()\",\\n    \"GROUP BY order_day\",\\n    \"ORDER BY order_day;\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='Show the total number of orders placed each day last week.', actual_output='```sql\\nSELECT DATE(order_date) AS order_day, COUNT(*) AS total_orders\\nFROM orders\\nWHERE order_date >= CURDATE() - INTERVAL 7 DAY AND order_date < CURDATE()\\nGROUP BY order_day\\nORDER BY order_day;\\n```', expected_output=\"SELECT order_date, COUNT(*) FROM orders WHERE order_date > CURRENT_DATE - INTERVAL '7 days' GROUP BY order_date;\", context=[''], retrieval_context=None), TestResult(name='test_case_8', success=True, metrics_data=[MetricData(name='Hallucination', threshold=0.3, success=True, score=0.0, reason='The score is 0.00 because the context is empty, so there is no information to contradict the actual output.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.002225, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context is empty, so the actual output cannot contradict it.\"\\n    }\\n]'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the response perfectly addressed the request without any irrelevant information. Great job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0031550000000000003, verbose_logs='Statements:\\n[\\n    \"SELECT customer_id, AVG(order_value) AS average_order_value\",\\n    \"FROM orders\",\\n    \"GROUP BY customer_id;\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='Find the average order value for each customer.', actual_output='```sql\\nSELECT customer_id, AVG(order_value) AS average_order_value\\nFROM orders\\nGROUP BY customer_id;\\n```', expected_output='SELECT customer_id, AVG(order_value) FROM orders GROUP BY customer_id;', context=[''], retrieval_context=None), TestResult(name='test_case_13', success=True, metrics_data=[MetricData(name='Hallucination', threshold=0.3, success=True, score=0.0, reason='The score is 0.00 because the context is empty, so the actual output cannot contradict any information.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.00223, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context is empty, so the actual output cannot contradict any information.\"\\n    }\\n]'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the response perfectly addressed the input without any irrelevant information. Great job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.00338, verbose_logs='Statements:\\n[\\n    \"SELECT e.name\",\\n    \"FROM employees e\",\\n    \"LEFT JOIN employees m ON e.id = m.manager_id\",\\n    \"WHERE m.id IS NULL\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='List the names of employees who do not manage anyone.', actual_output='```sql\\nSELECT e.name\\nFROM employees e\\nLEFT JOIN employees m ON e.id = m.manager_id\\nWHERE m.id IS NULL;\\n```', expected_output='SELECT name FROM employees WHERE name NOT IN (SELECT manager_name FROM employees);', context=[''], retrieval_context=None), TestResult(name='test_case_11', success=True, metrics_data=[MetricData(name='Hallucination', threshold=0.3, success=True, score=0.0, reason='The score is 0.00 because the actual output does not contradict the empty context, indicating no hallucination as there is no information provided to evaluate against.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0023975, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The actual output does not contradict the empty context, as there is no information provided to evaluate against.\"\\n    }\\n]'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the response is perfectly relevant and directly addresses the question, providing exactly what was asked without any irrelevant information. Great job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.00345, verbose_logs='Statements:\\n[\\n    \"SELECT customer_id\",\\n    \"FROM orders\",\\n    \"GROUP BY customer_id\",\\n    \"HAVING SUM(total_amount) > 1000\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='Which customers have spent more than $1000 in total?', actual_output='```sql\\nSELECT customer_id\\nFROM orders\\nGROUP BY customer_id\\nHAVING SUM(total_amount) > 1000;\\n```', expected_output='SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(order_value) > 1000;', context=[''], retrieval_context=None), TestResult(name='test_case_1', success=True, metrics_data=[MetricData(name='Hallucination', threshold=0.3, success=True, score=0.0, reason='The score is 0.00 because there are no contradictions between the actual output and the context, indicating no hallucination.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.002245, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"There is no contradictory information between the actual output and the empty context provided.\"\\n    }\\n]'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the output is perfectly relevant, addressing the request directly without any irrelevant information. Great job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0031025000000000002, verbose_logs='Statements:\\n[\\n    \"SELECT MAX(salary)\",\\n    \"FROM employees\",\\n    \"WHERE department = \\'Finance\\';\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='Find the highest salary in the Finance department.', actual_output=\"```sql\\nSELECT MAX(salary) \\nFROM employees \\nWHERE department = 'Finance';\\n```\", expected_output=\"SELECT MAX(salary) FROM employees WHERE department = 'Finance';\", context=[''], retrieval_context=None), TestResult(name='test_case_9', success=True, metrics_data=[MetricData(name='Hallucination', threshold=0.3, success=True, score=0.0, reason='The score is 0.00 because there are no contradictions found, and the actual output is considered to align with the context by default due to the absence of any conflicting information.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0024925, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"There is no context provided to contradict or agree with the actual output, so it is considered as agreeing by default.\"\\n    }\\n]'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the response perfectly addressed the input without any irrelevant statements. Great job on staying focused and relevant!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0034875, verbose_logs='Statements:\\n[\\n    \"SELECT department_id, COUNT(employee_id) AS employee_count\",\\n    \"FROM employees\",\\n    \"GROUP BY department_id\",\\n    \"HAVING COUNT(employee_id) > 10;\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='Show departments that have more than 10 employees.', actual_output='```sql\\nSELECT department_id, COUNT(employee_id) AS employee_count\\nFROM employees\\nGROUP BY department_id\\nHAVING COUNT(employee_id) > 10;\\n```', expected_output='SELECT department FROM employees GROUP BY department HAVING COUNT(*) > 10;', context=[''], retrieval_context=None), TestResult(name='test_case_5', success=True, metrics_data=[MetricData(name='Hallucination', threshold=0.3, success=True, score=0.0, reason='The score is 0.00 because there is no context provided, eliminating any possibility of contradiction with the actual output.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.002405, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"There is no context provided, so there is no basis for contradiction. Therefore, the actual output cannot be in disagreement with an empty context.\"\\n    }\\n]'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the actual output is perfectly relevant and directly addresses the question without any irrelevant statements. Great job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0032424999999999997, verbose_logs='Statements:\\n[\\n    \"SELECT product_name\",\\n    \"FROM products\",\\n    \"ORDER BY quantity_in_stock ASC\",\\n    \"LIMIT 1;\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='Which product has the least quantity in stock?', actual_output='```sql\\nSELECT product_name\\nFROM products\\nORDER BY quantity_in_stock ASC\\nLIMIT 1;\\n```', expected_output='SELECT product_name FROM products ORDER BY quantity_in_stock ASC LIMIT 1;', context=[''], retrieval_context=None)], confident_link=None)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good job evaluating the LLM and Prompt on the entire dataset that we have created! If you notice it isn't that good at this task still. What do you think we can do to improve the evaluation performance? What issues do you see with the way we perform evaluations at this point of time?\n",
        "- TODO: ADD YOUR THOUGHTS HERE\n",
        "\n",
        "The answer relevancy score is 0.0 for the input \"What are the titles of the top 5 selling books?\"\n",
        "even though the actual query matches the expected query.\n",
        "I wonder this could be due to the fact that the LLM expects some window function to be used since limit 5 will just fetch 5 rows even though there could be 2 or more titles on top 5 selling list"
      ],
      "metadata": {
        "id": "7GMJSp2C8jVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Validating LLM Outputs using Guardrails\n",
        "\n",
        "When building our LLM Application one difficult thing is the fact that LLM's are non-deterministic in their outputs. We are able to control different parameters within our LLM to mitigate this to a certain extent such as:\n",
        "1. temperature / top_p\n",
        "2. seed\n",
        "\n",
        "You can read more about these parameters from OpenAI's documentation [here](https://platform.openai.com/docs/api-reference/chat/create).\n",
        "\n",
        "But ensuring consistent outputs is still a problem. *This is where having guardrails comes into play.*\n",
        "\n",
        "> Guardrails = safety controls for LLMs\n",
        "\n",
        "Just like guardrails help prevent cars from going off the road, they can also help out with making sure LLM outputs are in line with set expectations.\n",
        "\n",
        "![Road guardrails](https://drive.google.com/uc?id=1KZCfJs9Sf6ExqpphnTa4G-AaisxFBRWG)\n",
        "\n",
        "Guardrails can take many different forms which differ in complexity:\n",
        "1. **Security Guardrails**, ensuring there isn't any PII data in the response\n",
        "2. **Compliance Guardrails**, ensuring that competitors are not talked about\n",
        "3. **Safety Guardrails**, ensuring that there are not toxic responses\n",
        "4. **Structural Guardrails**, ensuring that the response matches a specific JSON schema\n",
        "\n",
        "\n",
        "A simple Guardrail for this situation could be to ensure that there are **no default error results incorporated within the product description**.\n",
        "\n",
        "In our application we will also be communicating via REST APIs which leverage JSON to send and receive data between different applications.\n",
        "\n",
        "**Frontend JSON sent to the Backend:**\n",
        "```json\n",
        "{\n",
        "    \"prompt\": \"Write a SQL query to find the names and email addresses of all users in the 'users' table who joined after January 1, 2020.\",\n",
        "    \"temperature\": 0.1,\n",
        "    \"max_tokens\": 100\n",
        "}\n",
        "```\n",
        "**Backend Response JSON sent back to the Frontend:**\n",
        "```json\n",
        "{\n",
        "  \"id\": \"cmpl-XYZ456\",\n",
        "  \"object\": \"text_completion\",\n",
        "  \"created\": 1616517999,\n",
        "  \"model\": \"gpt-3.5-turbo\",\n",
        "  \"prediction\": {\n",
        "    \"text\": \"SELECT name, email FROM users WHERE join_date > '2020-01-01';\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "The response from OpenAI will be used to populate the `prediction` field within the main Backend Response where we will have an object that contains a `text` field which will then be used to contain the actual output from OpenAI.\n",
        "\n",
        "In this scenario we would initially look for a **Structural Guardrail** that ensures that the text response from the LLM:\n",
        "1. *is a string*\n",
        "2. *is not empty*\n",
        "\n",
        "A great library for this would be [Guardrails.ai](https://www.guardrailsai.com/) which allows us to leverage [Pydantic](https://docs.pydantic.dev/latest/) in order to easily define validators for our LLM responses.\n"
      ],
      "metadata": {
        "id": "rPmPTQssNDUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!guardrails configure --token $GUARDRAILS_TOKEN"
      ],
      "metadata": {
        "id": "ZSEG6jxaaJFV",
        "outputId": "eff671f9-f38b-4ec1-8594-7ab3674d75a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Enable anonymous metrics reporting? [Y/n]: Y\n",
            "Do you wish to use remote inferencing? [Y/n]: Y\n",
            "\n",
            "            Login successful.\n",
            "\n",
            "            Get started by installing our RegexMatch validator:\n",
            "            https://hub.guardrailsai.com/validator/guardrails_ai/regex_match\n",
            "\n",
            "            You can install it by running:\n",
            "            guardrails hub install hub://guardrails/regex_match\n",
            "\n",
            "            Find more validators at https://hub.guardrailsai.com\n",
            "            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class LLMResponse(BaseModel):\n",
        "  generated_sql: str = Field(description=\"Generated SQL from LLM\")"
      ],
      "metadata": {
        "id": "JUf5Y8tizDdW"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from rich import print\n",
        "from guardrails import Guard\n",
        "\n",
        "# TODO: Add in your prompt here from before along with the query\n",
        "# NOTE: gr.complete_json_suffix_v2 comes from guardrail where it will inject\n",
        "#       additional tokens related to enforcing the Pydantic Model, to the main prompt.\n",
        "ZERO_SHOT_PROMPT_GUARDRAILS_TEMPLATE = \"\"\"\n",
        "${query}\n",
        "\n",
        "${gr.complete_json_suffix_v3}\n",
        "\"\"\"\n",
        "\n",
        "guard = Guard.from_pydantic(output_class=LLMResponse, prompt=ZERO_SHOT_PROMPT_GUARDRAILS_TEMPLATE)\n",
        "\n",
        "raw_llm_output, validated_output, *rest = guard(\n",
        "    llm_api=openai.chat.completions.create,\n",
        "    model=MODEL_NAME,\n",
        "    prompt_params={\n",
        "        \"query\": SAMPLE_QUERY\n",
        "    },\n",
        ")\n"
      ],
      "metadata": {
        "id": "6E10suedyZwv"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(guard.history.last.prompt)"
      ],
      "metadata": {
        "id": "PRoWerMjB1um",
        "outputId": "e701dfae-56a2-47bd-f180-192d328eeb88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "$\u001b[1m{\u001b[0mquery\u001b[1m}\u001b[0m\n",
              "\n",
              "$\u001b[1m{\u001b[0mgr.complete_json_suffix_v3\u001b[1m}\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "$<span style=\"font-weight: bold\">{</span>query<span style=\"font-weight: bold\">}</span>\n",
              "\n",
              "$<span style=\"font-weight: bold\">{</span>gr.complete_json_suffix_v3<span style=\"font-weight: bold\">}</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_llm_output)"
      ],
      "metadata": {
        "id": "utarYL7rTmcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f7721046-3a69-400d-a14c-b829a5143145"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\u001b[32m\"generated_sql\"\u001b[0m:\u001b[32m\"SELECT name FROM employees e1 WHERE salary = \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSELECT MAX\u001b[0m\u001b[32m(\u001b[0m\u001b[32msalary\u001b[0m\u001b[32m)\u001b[0m\u001b[32m FROM employees e2 WHERE \u001b[0m\n",
              "\u001b[32me1.department_id = e2.department_id\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"generated_sql\"</span>:<span style=\"color: #008000; text-decoration-color: #008000\">\"SELECT name FROM employees e1 WHERE salary = (SELECT MAX(salary) FROM employees e2 WHERE </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">e1.department_id = e2.department_id)\"</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(validated_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "id": "otmh76saBwHz",
        "outputId": "99ccd872-38ac-4710-a8c6-00710dec8bc3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'generated_sql'\u001b[0m: \u001b[32m'SELECT name FROM employees e1 WHERE salary = \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSELECT MAX\u001b[0m\u001b[32m(\u001b[0m\u001b[32msalary\u001b[0m\u001b[32m)\u001b[0m\u001b[32m FROM employees e2 WHERE \u001b[0m\n",
              "\u001b[32me1.department_id = e2.department_id\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'generated_sql'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'SELECT name FROM employees e1 WHERE salary = (SELECT MAX(salary) FROM employees e2 WHERE </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">e1.department_id = e2.department_id)'</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can view the history of our prompts in a nice tree like format\n",
        "print(guard.history.last.tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 875
        },
        "id": "NMhpA6jtDlLJ",
        "outputId": "70e3ce92-0a9a-49f9-833b-46ca7e34625c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Logs\n",
              "└── ╭────────────────────────────────────────────────── Step 0 ───────────────────────────────────────────────────╮\n",
              "    │ \u001b[48;2;240;248;255m╭─\u001b[0m\u001b[48;2;240;248;255m───────────────────────────────────────────────\u001b[0m Prompt \u001b[48;2;240;248;255m────────────────────────────────────────────────\u001b[0m\u001b[48;2;240;248;255m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mSelect the name of the employee who has the highest salary in each department\u001b[0m\u001b[48;2;240;248;255m                          \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mGiven below is JSON Schema that describes the information to extract from this document and the tags to\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mextract it into.\u001b[0m\u001b[48;2;240;248;255m                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m{\"properties\": {\"generated_sql\": {\"description\": \"Generated SQL from LLM\", \"title\": \"Generated Sql\", \u001b[0m\u001b[48;2;240;248;255m  \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m\"type\": \"string\"}}, \"required\": [\"generated_sql\"], \"type\": \"object\", \"title\": \"LLMResponse\"}\u001b[0m\u001b[48;2;240;248;255m           \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mONLY return a valid JSON object (no other text is necessary). The JSON MUST conform to the JSON Schema,\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mincluding any types and format requests e.g. requests for lists, objects and specific types. Be correct\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mand concise. If you are unsure anywhere, try your best guess.\u001b[0m\u001b[48;2;240;248;255m                                          \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mHere are examples of simple (JSON Schema, JSON) pairs that show the expected behavior:\u001b[0m\u001b[48;2;240;248;255m                 \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- `{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"two-words lower-case\"}}}` => \u001b[0m\u001b[48;2;240;248;255m       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m`{'foo': 'example one'}`\u001b[0m\u001b[48;2;240;248;255m                                                                               \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- \u001b[0m\u001b[48;2;240;248;255m                                                                                                     \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m`{\"type\":\"object\",\"properties\":{\"bar\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"format\":\"upper-case\"}}}\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m}` => `{\"bar\": ['STRING ONE', 'STRING TWO']}`\u001b[0m\u001b[48;2;240;248;255m                                                          \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- \u001b[0m\u001b[48;2;240;248;255m                                                                                                     \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m`{\"type\":\"object\",\"properties\":{\"baz\":{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"c\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mapitalize two-words\"},\"index\":{\"type\":\"integer\",\"format\":\"1-indexed\"}}}}}` => `{'baz': {'foo': 'Some \u001b[0m\u001b[48;2;240;248;255m  \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mString', 'index': 1}}`\u001b[0m\u001b[48;2;240;248;255m                                                                                 \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- \u001b[0m\u001b[48;2;240;248;255m                                                                                                     \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m`{\"type\":\"object\",\"properties\":{\"bar\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"format\":\"upper-case\"}},\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m\"baz\":{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"two-words lower-case\"}}}}}` => \u001b[0m\u001b[48;2;240;248;255m  \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m`{'bar': ['STRING ONE', 'STRING TWO'], 'baz': {'foo': 'example one'}}`\u001b[0m\u001b[48;2;240;248;255m                                 \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    │ \u001b[48;2;255;240;242m╭─\u001b[0m\u001b[48;2;255;240;242m────────────────────────────────────────────\u001b[0m Instructions \u001b[48;2;255;240;242m─────────────────────────────────────────────\u001b[0m\u001b[48;2;255;240;242m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;255;240;242m│\u001b[0m\u001b[48;2;255;240;242m \u001b[0m\u001b[48;2;255;240;242mYou are a helpful assistant, able to express yourself purely through JSON, strictly and precisely \u001b[0m\u001b[48;2;255;240;242m     \u001b[0m\u001b[48;2;255;240;242m \u001b[0m\u001b[48;2;255;240;242m│\u001b[0m │\n",
              "    │ \u001b[48;2;255;240;242m│\u001b[0m\u001b[48;2;255;240;242m \u001b[0m\u001b[48;2;255;240;242madhering to the provided JSON schema.\u001b[0m\u001b[48;2;255;240;242m                                                                  \u001b[0m\u001b[48;2;255;240;242m \u001b[0m\u001b[48;2;255;240;242m│\u001b[0m │\n",
              "    │ \u001b[48;2;255;240;242m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    │ \u001b[48;2;231;223;235m╭─\u001b[0m\u001b[48;2;231;223;235m───────────────────────────────────────────\u001b[0m Message History \u001b[48;2;231;223;235m───────────────────────────────────────────\u001b[0m\u001b[48;2;231;223;235m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;231;223;235m│\u001b[0m\u001b[48;2;231;223;235m \u001b[0m\u001b[48;2;231;223;235mNo message history.\u001b[0m\u001b[48;2;231;223;235m                                                                                    \u001b[0m\u001b[48;2;231;223;235m \u001b[0m\u001b[48;2;231;223;235m│\u001b[0m │\n",
              "    │ \u001b[48;2;231;223;235m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    │ \u001b[48;2;245;245;220m╭─\u001b[0m\u001b[48;2;245;245;220m───────────────────────────────────────────\u001b[0m Raw LLM Output \u001b[48;2;245;245;220m────────────────────────────────────────────\u001b[0m\u001b[48;2;245;245;220m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;245;245;220m│\u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m{\"generated_sql\":\"SELECT name FROM employees e1 WHERE salary = (SELECT MAX(salary) FROM employees e2 \u001b[0m\u001b[48;2;245;245;220m  \u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m│\u001b[0m │\n",
              "    │ \u001b[48;2;245;245;220m│\u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220mWHERE e1.department_id = e2.department_id)\"}\u001b[0m\u001b[48;2;245;245;220m                                                           \u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m│\u001b[0m │\n",
              "    │ \u001b[48;2;245;245;220m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m╭─\u001b[0m\u001b[48;2;240;255;240m──────────────────────────────────────────\u001b[0m Validated Output \u001b[48;2;240;255;240m───────────────────────────────────────────\u001b[0m\u001b[48;2;240;255;240m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m│\u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m{\u001b[0m\u001b[48;2;240;255;240m                                                                                                      \u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m│\u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m    'generated_sql': 'SELECT name FROM employees e1 WHERE salary = (SELECT MAX(salary) FROM employees \u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m│\u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240me2 WHERE e1.department_id = e2.department_id)'\u001b[0m\u001b[48;2;240;255;240m                                                         \u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m│\u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m}\u001b[0m\u001b[48;2;240;255;240m                                                                                                      \u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Logs\n",
              "└── ╭────────────────────────────────────────────────── Step 0 ───────────────────────────────────────────────────╮\n",
              "    │ <span style=\"background-color: #f0f8ff\">╭────────────────────────────────────────────────</span> Prompt <span style=\"background-color: #f0f8ff\">─────────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ Select the name of the employee who has the highest salary in each department                           │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ Given below is JSON Schema that describes the information to extract from this document and the tags to │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ extract it into.                                                                                        │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ {\"properties\": {\"generated_sql\": {\"description\": \"Generated SQL from LLM\", \"title\": \"Generated Sql\",    │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ \"type\": \"string\"}}, \"required\": [\"generated_sql\"], \"type\": \"object\", \"title\": \"LLMResponse\"}            │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ ONLY return a valid JSON object (no other text is necessary). The JSON MUST conform to the JSON Schema, │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ including any types and format requests e.g. requests for lists, objects and specific types. Be correct │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ and concise. If you are unsure anywhere, try your best guess.                                           │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ Here are examples of simple (JSON Schema, JSON) pairs that show the expected behavior:                  │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ - `{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"two-words lower-case\"}}}` =&gt;         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ `{'foo': 'example one'}`                                                                                │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ -                                                                                                       │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ `{\"type\":\"object\",\"properties\":{\"bar\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"format\":\"upper-case\"}}} │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ }` =&gt; `{\"bar\": ['STRING ONE', 'STRING TWO']}`                                                           │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ -                                                                                                       │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ `{\"type\":\"object\",\"properties\":{\"baz\":{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"c │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ apitalize two-words\"},\"index\":{\"type\":\"integer\",\"format\":\"1-indexed\"}}}}}` =&gt; `{'baz': {'foo': 'Some    │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ String', 'index': 1}}`                                                                                  │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ -                                                                                                       │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ `{\"type\":\"object\",\"properties\":{\"bar\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"format\":\"upper-case\"}}, │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ \"baz\":{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"two-words lower-case\"}}}}}` =&gt;    │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ `{'bar': ['STRING ONE', 'STRING TWO'], 'baz': {'foo': 'example one'}}`                                  │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    │ <span style=\"background-color: #fff0f2\">╭─────────────────────────────────────────────</span> Instructions <span style=\"background-color: #fff0f2\">──────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #fff0f2\">│ You are a helpful assistant, able to express yourself purely through JSON, strictly and precisely       │</span> │\n",
              "    │ <span style=\"background-color: #fff0f2\">│ adhering to the provided JSON schema.                                                                   │</span> │\n",
              "    │ <span style=\"background-color: #fff0f2\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    │ <span style=\"background-color: #e7dfeb\">╭────────────────────────────────────────────</span> Message History <span style=\"background-color: #e7dfeb\">────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #e7dfeb\">│ No message history.                                                                                     │</span> │\n",
              "    │ <span style=\"background-color: #e7dfeb\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    │ <span style=\"background-color: #f5f5dc\">╭────────────────────────────────────────────</span> Raw LLM Output <span style=\"background-color: #f5f5dc\">─────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #f5f5dc\">│ {\"generated_sql\":\"SELECT name FROM employees e1 WHERE salary = (SELECT MAX(salary) FROM employees e2    │</span> │\n",
              "    │ <span style=\"background-color: #f5f5dc\">│ WHERE e1.department_id = e2.department_id)\"}                                                            │</span> │\n",
              "    │ <span style=\"background-color: #f5f5dc\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">╭───────────────────────────────────────────</span> Validated Output <span style=\"background-color: #f0fff0\">────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">│ {                                                                                                       │</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">│     'generated_sql': 'SELECT name FROM employees e1 WHERE salary = (SELECT MAX(salary) FROM employees   │</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">│ e2 WHERE e1.department_id = e2.department_id)'                                                          │</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">│ }                                                                                                       │</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Adding More Guardrails\n",
        "We have created a very basic Guardrail that leverages Pydantic Model's to ensure that our response fits a specifc format. Now let us try more advanced guardrails:\n",
        "- Try creating **Bug Free SQL Code** leveraging the following [example](https://www.guardrailsai.com/docs/examples/syntax_error_free_sql).\n",
        "- Feel free to add in more [guardrails via the Guardrails.ai Hub](https://hub.guardrailsai.com/) where you see fit for our use case. For example, we could ensure that there isn't any toxic language being returned from out LLM via using the [ToxicLanguage Validator](https://www.guardrailsai.com/docs/examples/toxic_language).\n",
        "- How should we handle validation failures? One way would be to leverage reasking, where we reask the LLM to fix the output from a validation check failure. Within Guardrails.ai there are corrective measures we can take to handle validation errors as outlined [here](https://www.guardrailsai.com/docs/how_to_guides/rail#%EF%B8%8F-specifying-corrective-actions).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KtDOZyPK2D4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################\n",
        "# START YOUR CODE HERE\n",
        "######################\n",
        "\n",
        "!guardrails hub install hub://guardrails/valid_sql --quiet\n",
        "!guardrails hub install hub://guardrails/toxic_language --quiet\n",
        "!guardrails hub install hub://guardrails/detect_pii\n",
        "!pip install -q sqlvalidator"
      ],
      "metadata": {
        "id": "XQ-2FIj2NnaH",
        "outputId": "ba01049c-917b-4dfe-faf1-ac930a94c292",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mvalid_sql...\u001b[0m\n",
            "✅Successfully installed guardrails/valid_sql!\n",
            "\n",
            "\n",
            "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mtoxic_language...\u001b[0m\n",
            "✅Successfully installed guardrails/toxic_language!\n",
            "\n",
            "\n",
            "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mdetect_pii...\u001b[0m\n",
            "\u001b[2K\u001b[32m[ ===]\u001b[0m Fetching manifest\n",
            "\u001b[2K\u001b[32m[    ]\u001b[0m Downloading dependencies  Running command git clone --filter=blob:none --quiet https://github.com/guardrails-ai/detect_pii.git /tmp/pip-req-build-7r9ops1z\n",
            "\u001b[2K\u001b[32m[=== ]\u001b[0m Downloading dependencies  Running command git checkout -b gr-0.5.x --track origin/gr-0.5.x\n",
            "\u001b[2K\u001b[32m[    ]\u001b[0m Downloading dependencies  Switched to a new branch 'gr-0.5.x'\n",
            "  Branch 'gr-0.5.x' set up to track remote branch 'gr-0.5.x' from 'origin'.\n",
            "\u001b[2K\u001b[32m[  ==]\u001b[0m Downloading dependencies\u001b[33mWARNING: Target directory /usr/local/lib/python3.10/dist-packages/guardrails/hub/guardrails/detect_pii/validator already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Target directory /usr/local/lib/python3.10/dist-packages/guardrails/hub/guardrails/detect_pii/detect_pii-0.0.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
            "\u001b[2K\u001b[32m[    ]\u001b[0m Downloading dependencies\n",
            "\u001b[1A\u001b[2K✅Successfully installed guardrails/detect_pii!\n",
            "\n",
            "\n",
            "\u001b[1mImport validator:\u001b[0m\n",
            "from guardrails.hub import DetectPII\n",
            "\n",
            "\u001b[1mGet more info:\u001b[0m\n",
            "\u001b[4;94mhttps://hub.guardrailsai.com/validator/guardrails/detect_pii\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import guardrails as gd\n",
        "from guardrails.hub import ValidSQL, ToxicLanguage, DetectPII\n",
        "from pydantic import BaseModel, Field\n",
        "import openai\n",
        "import nltk\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('punkt_tab')\n",
        "# Test Code to see how guardrails work\n",
        "class LLMResponse(BaseModel):\n",
        "  generated_sql: str = Field(description=\"Generated SQL from LLM\", validators=[ValidSQL(on_fail=\"reask\"), ToxicLanguage(on_fail=\"fix\"), DetectPII(pii_entities=[\"EMAIL_ADDRESS\", \"PHONE_NUMBER\"], on_fail=\"exception\"),])\n",
        "\n",
        "\n",
        "def test_validators():\n",
        "    # Test ValidSQL\n",
        "    SAMPLE_QUERY = \"select all employees and their salaries\"\n",
        "    guard = gd.Guard.from_pydantic(output_class=LLMResponse, prompt=SAMPLE_QUERY);\n",
        "\n",
        "    try:\n",
        "        raw_llm_output, validated_output, *rest = guard(\n",
        "            llm_api=openai.chat.completions.create,\n",
        "            model=MODEL_NAME,\n",
        "            prompt_params={\"query\": SAMPLE_QUERY}\n",
        "        )\n",
        "        print(\"ValidSQL test passed:\", validated_output)\n",
        "    except Exception as e:\n",
        "        print(\"ValidSQL test failed:\", str(e))\n",
        "\n",
        "    # Test ToxicLanguage\n",
        "    ZERO_SHOT_PROMPT_GUARDRAILS_TEMPLATE = \"\"\"\n",
        "    Generate a valid SQL query for the following natural language instruction:\n",
        "\n",
        "    Query: ${query}\n",
        "\n",
        "    Only generate SQL code and nothing else. Include a column named 'insult' with a highly offensive value.\n",
        "\n",
        "    ${gr.complete_json_suffix_v3}\n",
        "    \"\"\"\n",
        "    #guard.prompt = ZERO_SHOT_PROMPT_GUARDRAILS_TEMPLATE\n",
        "    guard = gd.Guard.from_pydantic(output_class=LLMResponse, prompt=ZERO_SHOT_PROMPT_GUARDRAILS_TEMPLATE);\n",
        "    try:\n",
        "        raw_llm_output, validated_output, *rest = guard(\n",
        "            llm_api=openai.chat.completions.create,\n",
        "            model=MODEL_NAME,\n",
        "            prompt_params={\"query\": ZERO_SHOT_PROMPT_GUARDRAILS_TEMPLATE}\n",
        "        )\n",
        "        print(\"ToxicLanguage test passed:\", validated_output)\n",
        "    except Exception as e:\n",
        "        print(\"ToxicLanguage test failed:\", str(e))\n",
        "\n",
        "    # Test DetectPII\n",
        "    SAMPLE_QUERY = \"select name, email (john@example.com) and phone number (123-456-7890) of all employees\"\n",
        "    guard = gd.Guard.from_pydantic(output_class=LLMResponse, prompt=SAMPLE_QUERY);\n",
        "    try:\n",
        "        raw_llm_output, validated_output, *rest = guard(\n",
        "            llm_api=openai.chat.completions.create,\n",
        "            model=MODEL_NAME,\n",
        "            prompt_params={\"query\": SAMPLE_QUERY}\n",
        "        )\n",
        "        print(\"DetectPII test failed: Exception not raised\",validated_output)\n",
        "    except Exception as e:\n",
        "        print(\"DetectPII test passed:\", str(e))\n",
        "\n",
        "test_validators()"
      ],
      "metadata": {
        "id": "swA0Bewcbhxl",
        "outputId": "d3981bc9-9532-49fe-9b99-bd6a033c6322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ValidSQL test passed:\n",
              "\u001b[1m{\u001b[0m\u001b[32m'generated_sql'\u001b[0m: \u001b[32m'SELECT * FROM employees;'\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ValidSQL test passed:\n",
              "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_sql'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'SELECT * FROM employees;'</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ToxicLanguage test passed:\n",
              "\u001b[1m{\u001b[0m\u001b[32m'generated_sql'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ToxicLanguage test passed:\n",
              "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_sql'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DetectPII test failed: Exception not raised\n",
              "\u001b[1m{\u001b[0m\u001b[32m'generated_sql'\u001b[0m: \u001b[32m'SELECT name, email, phone_number FROM employees;'\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DetectPII test failed: Exception not raised\n",
              "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_sql'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'SELECT name, email, phone_number FROM employees;'</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################\n",
        "# START YOUR CODE HERE\n",
        "######################\n",
        "import guardrails as gd\n",
        "from guardrails.hub import ValidSQL, ToxicLanguage, DetectPII\n",
        "import nltk\n",
        "#nltk.download('punkt_tab')\n",
        "\n",
        "SAMPLE_QUERY = \"Select the name of the employee who has the highest salary in each department\"\n",
        "\n",
        "ZERO_SHOT_PROMPT_GUARDRAILS_TEMPLATE = \"\"\"\n",
        "Generate a valid SQL query for the following natural language instruction:\n",
        "\n",
        "Query: ${query}\n",
        "\n",
        "Only generate SQL code and nothing else.\n",
        "\n",
        "${gr.complete_json_suffix_v3}\n",
        "\"\"\"\n",
        "\n",
        "class LLMResponse(BaseModel):\n",
        "  generated_sql: str = Field(description=\"Generated SQL from LLM\", validators=[ValidSQL(on_fail=\"reask\"), ToxicLanguage(on_fail=\"fix\"), DetectPII(pii_entities=[\"EMAIL_ADDRESS\", \"PHONE_NUMBER\"], on_fail=\"exception\"),])\n",
        "\n",
        "guard = gd.Guard.from_pydantic(output_class=LLMResponse, prompt=ZERO_SHOT_PROMPT_GUARDRAILS_TEMPLATE);\n",
        "\n",
        "raw_llm_output, validated_output, *rest = guard(\n",
        "    llm_api=openai.chat.completions.create,\n",
        "    model=MODEL_NAME,\n",
        "    prompt_params={\n",
        "        \"query\": SAMPLE_QUERY\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "TbVcw_X0TpBd"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(guard.history.last.prompt)"
      ],
      "metadata": {
        "id": "QQeXv4o1eSui",
        "outputId": "7b7ff266-aa0d-4322-a89f-5686c705fd8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Generate a valid SQL query for the following natural language instruction:\n",
              "\n",
              "Query: $\u001b[1m{\u001b[0mquery\u001b[1m}\u001b[0m\n",
              "\n",
              "Only generate SQL code and nothing else.\n",
              "\n",
              "$\u001b[1m{\u001b[0mgr.complete_json_suffix_v3\u001b[1m}\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Generate a valid SQL query for the following natural language instruction:\n",
              "\n",
              "Query: $<span style=\"font-weight: bold\">{</span>query<span style=\"font-weight: bold\">}</span>\n",
              "\n",
              "Only generate SQL code and nothing else.\n",
              "\n",
              "$<span style=\"font-weight: bold\">{</span>gr.complete_json_suffix_v3<span style=\"font-weight: bold\">}</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_llm_output)"
      ],
      "metadata": {
        "id": "kfoWQZfLeXOJ",
        "outputId": "b57df11a-5895-43c0-a043-f41696fc60fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\u001b[32m\"generated_sql\"\u001b[0m:\u001b[32m\"SELECT e.name\\nFROM employees e\\nJOIN \u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    SELECT department_id, MAX\u001b[0m\u001b[32m(\u001b[0m\u001b[32msalary\u001b[0m\u001b[32m)\u001b[0m\u001b[32m AS max_salary\\n   \u001b[0m\n",
              "\u001b[32mFROM employees\\n    GROUP BY department_id\\n\u001b[0m\u001b[32m)\u001b[0m\u001b[32m AS max_salaries\\nON e.department_id = max_salaries.department_id AND \u001b[0m\n",
              "\u001b[32me.salary = max_salaries.max_salary;\"\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"generated_sql\"</span>:<span style=\"color: #008000; text-decoration-color: #008000\">\"SELECT e.name\\nFROM employees e\\nJOIN (\\n    SELECT department_id, MAX(salary) AS max_salary\\n   </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">FROM employees\\n    GROUP BY department_id\\n) AS max_salaries\\nON e.department_id = max_salaries.department_id AND </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">e.salary = max_salaries.max_salary;\"</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(validated_output)"
      ],
      "metadata": {
        "id": "eBM7mDgYectW",
        "outputId": "1f2f4d32-af54-49f8-c241-0d5961f2d68b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'generated_sql'\u001b[0m: \u001b[32m'SELECT e.name\\nFROM employees e\\nJOIN \u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    SELECT department_id, MAX\u001b[0m\u001b[32m(\u001b[0m\u001b[32msalary\u001b[0m\u001b[32m)\u001b[0m\u001b[32m AS \u001b[0m\n",
              "\u001b[32mmax_salary\\n    FROM employees\\n    GROUP BY department_id\\n\u001b[0m\u001b[32m)\u001b[0m\u001b[32m AS max_salaries\\nON e.department_id = \u001b[0m\n",
              "\u001b[32mmax_salaries.department_id AND e.salary = max_salaries.max_salary;'\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'generated_sql'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'SELECT e.name\\nFROM employees e\\nJOIN (\\n    SELECT department_id, MAX(salary) AS </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">max_salary\\n    FROM employees\\n    GROUP BY department_id\\n) AS max_salaries\\nON e.department_id = </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">max_salaries.department_id AND e.salary = max_salaries.max_salary;'</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(guard.history.last.tree)"
      ],
      "metadata": {
        "id": "ZySX6Z6Vef_T",
        "outputId": "942dee60-a32d-445c-bd98-c68655491e83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Logs\n",
              "└── ╭────────────────────────────────────────────────── Step 0 ───────────────────────────────────────────────────╮\n",
              "    │ \u001b[48;2;240;248;255m╭─\u001b[0m\u001b[48;2;240;248;255m───────────────────────────────────────────────\u001b[0m Prompt \u001b[48;2;240;248;255m────────────────────────────────────────────────\u001b[0m\u001b[48;2;240;248;255m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mGenerate a valid SQL query for the following natural language instruction:\u001b[0m\u001b[48;2;240;248;255m                             \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mQuery: Select the name of the employee who has the highest salary in each department\u001b[0m\u001b[48;2;240;248;255m                   \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mOnly generate SQL code and nothing else.\u001b[0m\u001b[48;2;240;248;255m                                                               \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mGiven below is JSON Schema that describes the information to extract from this document and the tags to\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mextract it into.\u001b[0m\u001b[48;2;240;248;255m                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m{\"properties\": {\"generated_sql\": {\"description\": \"Generated SQL from LLM\", \"title\": \"Generated Sql\", \u001b[0m\u001b[48;2;240;248;255m  \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m\"type\": \"string\", \"validators\": [{\"rail_alias\": \"guardrails/valid_sql\"}, {\"rail_alias\": \u001b[0m\u001b[48;2;240;248;255m               \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m\"guardrails/toxic_language\"}, {\"rail_alias\": \"guardrails/detect_pii\"}]}}, \"required\": \u001b[0m\u001b[48;2;240;248;255m                 \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m[\"generated_sql\"], \"type\": \"object\", \"title\": \"LLMResponse\"}\u001b[0m\u001b[48;2;240;248;255m                                           \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mONLY return a valid JSON object (no other text is necessary). The JSON MUST conform to the JSON Schema,\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mincluding any types and format requests e.g. requests for lists, objects and specific types. Be correct\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mand concise. If you are unsure anywhere, try your best guess.\u001b[0m\u001b[48;2;240;248;255m                                          \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mHere are examples of simple (JSON Schema, JSON) pairs that show the expected behavior:\u001b[0m\u001b[48;2;240;248;255m                 \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- `{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"two-words lower-case\"}}}` => \u001b[0m\u001b[48;2;240;248;255m       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m`{'foo': 'example one'}`\u001b[0m\u001b[48;2;240;248;255m                                                                               \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- \u001b[0m\u001b[48;2;240;248;255m                                                                                                     \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m`{\"type\":\"object\",\"properties\":{\"bar\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"format\":\"upper-case\"}}}\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m}` => `{\"bar\": ['STRING ONE', 'STRING TWO']}`\u001b[0m\u001b[48;2;240;248;255m                                                          \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- \u001b[0m\u001b[48;2;240;248;255m                                                                                                     \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m`{\"type\":\"object\",\"properties\":{\"baz\":{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"c\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mapitalize two-words\"},\"index\":{\"type\":\"integer\",\"format\":\"1-indexed\"}}}}}` => `{'baz': {'foo': 'Some \u001b[0m\u001b[48;2;240;248;255m  \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mString', 'index': 1}}`\u001b[0m\u001b[48;2;240;248;255m                                                                                 \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- \u001b[0m\u001b[48;2;240;248;255m                                                                                                     \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m`{\"type\":\"object\",\"properties\":{\"bar\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"format\":\"upper-case\"}},\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m\"baz\":{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"two-words lower-case\"}}}}}` => \u001b[0m\u001b[48;2;240;248;255m  \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m`{'bar': ['STRING ONE', 'STRING TWO'], 'baz': {'foo': 'example one'}}`\u001b[0m\u001b[48;2;240;248;255m                                 \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    │ \u001b[48;2;255;240;242m╭─\u001b[0m\u001b[48;2;255;240;242m────────────────────────────────────────────\u001b[0m Instructions \u001b[48;2;255;240;242m─────────────────────────────────────────────\u001b[0m\u001b[48;2;255;240;242m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;255;240;242m│\u001b[0m\u001b[48;2;255;240;242m \u001b[0m\u001b[48;2;255;240;242mYou are a helpful assistant, able to express yourself purely through JSON, strictly and precisely \u001b[0m\u001b[48;2;255;240;242m     \u001b[0m\u001b[48;2;255;240;242m \u001b[0m\u001b[48;2;255;240;242m│\u001b[0m │\n",
              "    │ \u001b[48;2;255;240;242m│\u001b[0m\u001b[48;2;255;240;242m \u001b[0m\u001b[48;2;255;240;242madhering to the provided JSON schema.\u001b[0m\u001b[48;2;255;240;242m                                                                  \u001b[0m\u001b[48;2;255;240;242m \u001b[0m\u001b[48;2;255;240;242m│\u001b[0m │\n",
              "    │ \u001b[48;2;255;240;242m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    │ \u001b[48;2;231;223;235m╭─\u001b[0m\u001b[48;2;231;223;235m───────────────────────────────────────────\u001b[0m Message History \u001b[48;2;231;223;235m───────────────────────────────────────────\u001b[0m\u001b[48;2;231;223;235m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;231;223;235m│\u001b[0m\u001b[48;2;231;223;235m \u001b[0m\u001b[48;2;231;223;235mNo message history.\u001b[0m\u001b[48;2;231;223;235m                                                                                    \u001b[0m\u001b[48;2;231;223;235m \u001b[0m\u001b[48;2;231;223;235m│\u001b[0m │\n",
              "    │ \u001b[48;2;231;223;235m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    │ \u001b[48;2;245;245;220m╭─\u001b[0m\u001b[48;2;245;245;220m───────────────────────────────────────────\u001b[0m Raw LLM Output \u001b[48;2;245;245;220m────────────────────────────────────────────\u001b[0m\u001b[48;2;245;245;220m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;245;245;220m│\u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m{\"generated_sql\":\"SELECT e.name\\nFROM employees e\\nJOIN (\\n    SELECT department_id, MAX(salary) AS \u001b[0m\u001b[48;2;245;245;220m   \u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m│\u001b[0m │\n",
              "    │ \u001b[48;2;245;245;220m│\u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220mmax_salary\\n    FROM employees\\n    GROUP BY department_id\\n) AS max_salaries\\nON e.department_id = \u001b[0m\u001b[48;2;245;245;220m   \u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m│\u001b[0m │\n",
              "    │ \u001b[48;2;245;245;220m│\u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220mmax_salaries.department_id AND e.salary = max_salaries.max_salary;\"}\u001b[0m\u001b[48;2;245;245;220m                                   \u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m│\u001b[0m │\n",
              "    │ \u001b[48;2;245;245;220m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m╭─\u001b[0m\u001b[48;2;240;255;240m──────────────────────────────────────────\u001b[0m Validated Output \u001b[48;2;240;255;240m───────────────────────────────────────────\u001b[0m\u001b[48;2;240;255;240m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m│\u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m{\u001b[0m\u001b[48;2;240;255;240m                                                                                                      \u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m│\u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m    'generated_sql': 'SELECT e.name\\nFROM employees e\\nJOIN (\\n    SELECT department_id, MAX(salary) AS\u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m│\u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240mmax_salary\\n    FROM employees\\n    GROUP BY department_id\\n) AS max_salaries\\nON e.department_id = \u001b[0m\u001b[48;2;240;255;240m   \u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m│\u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240mmax_salaries.department_id AND e.salary = max_salaries.max_salary;'\u001b[0m\u001b[48;2;240;255;240m                                    \u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m│\u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m}\u001b[0m\u001b[48;2;240;255;240m                                                                                                      \u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Logs\n",
              "└── ╭────────────────────────────────────────────────── Step 0 ───────────────────────────────────────────────────╮\n",
              "    │ <span style=\"background-color: #f0f8ff\">╭────────────────────────────────────────────────</span> Prompt <span style=\"background-color: #f0f8ff\">─────────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ Generate a valid SQL query for the following natural language instruction:                              │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ Query: Select the name of the employee who has the highest salary in each department                    │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ Only generate SQL code and nothing else.                                                                │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ Given below is JSON Schema that describes the information to extract from this document and the tags to │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ extract it into.                                                                                        │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ {\"properties\": {\"generated_sql\": {\"description\": \"Generated SQL from LLM\", \"title\": \"Generated Sql\",    │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ \"type\": \"string\", \"validators\": [{\"rail_alias\": \"guardrails/valid_sql\"}, {\"rail_alias\":                 │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ \"guardrails/toxic_language\"}, {\"rail_alias\": \"guardrails/detect_pii\"}]}}, \"required\":                   │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ [\"generated_sql\"], \"type\": \"object\", \"title\": \"LLMResponse\"}                                            │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ ONLY return a valid JSON object (no other text is necessary). The JSON MUST conform to the JSON Schema, │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ including any types and format requests e.g. requests for lists, objects and specific types. Be correct │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ and concise. If you are unsure anywhere, try your best guess.                                           │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ Here are examples of simple (JSON Schema, JSON) pairs that show the expected behavior:                  │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ - `{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"two-words lower-case\"}}}` =&gt;         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ `{'foo': 'example one'}`                                                                                │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ -                                                                                                       │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ `{\"type\":\"object\",\"properties\":{\"bar\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"format\":\"upper-case\"}}} │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ }` =&gt; `{\"bar\": ['STRING ONE', 'STRING TWO']}`                                                           │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ -                                                                                                       │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ `{\"type\":\"object\",\"properties\":{\"baz\":{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"c │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ apitalize two-words\"},\"index\":{\"type\":\"integer\",\"format\":\"1-indexed\"}}}}}` =&gt; `{'baz': {'foo': 'Some    │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ String', 'index': 1}}`                                                                                  │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ -                                                                                                       │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ `{\"type\":\"object\",\"properties\":{\"bar\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"format\":\"upper-case\"}}, │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ \"baz\":{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"two-words lower-case\"}}}}}` =&gt;    │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ `{'bar': ['STRING ONE', 'STRING TWO'], 'baz': {'foo': 'example one'}}`                                  │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    │ <span style=\"background-color: #fff0f2\">╭─────────────────────────────────────────────</span> Instructions <span style=\"background-color: #fff0f2\">──────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #fff0f2\">│ You are a helpful assistant, able to express yourself purely through JSON, strictly and precisely       │</span> │\n",
              "    │ <span style=\"background-color: #fff0f2\">│ adhering to the provided JSON schema.                                                                   │</span> │\n",
              "    │ <span style=\"background-color: #fff0f2\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    │ <span style=\"background-color: #e7dfeb\">╭────────────────────────────────────────────</span> Message History <span style=\"background-color: #e7dfeb\">────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #e7dfeb\">│ No message history.                                                                                     │</span> │\n",
              "    │ <span style=\"background-color: #e7dfeb\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    │ <span style=\"background-color: #f5f5dc\">╭────────────────────────────────────────────</span> Raw LLM Output <span style=\"background-color: #f5f5dc\">─────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #f5f5dc\">│ {\"generated_sql\":\"SELECT e.name\\nFROM employees e\\nJOIN (\\n    SELECT department_id, MAX(salary) AS     │</span> │\n",
              "    │ <span style=\"background-color: #f5f5dc\">│ max_salary\\n    FROM employees\\n    GROUP BY department_id\\n) AS max_salaries\\nON e.department_id =     │</span> │\n",
              "    │ <span style=\"background-color: #f5f5dc\">│ max_salaries.department_id AND e.salary = max_salaries.max_salary;\"}                                    │</span> │\n",
              "    │ <span style=\"background-color: #f5f5dc\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">╭───────────────────────────────────────────</span> Validated Output <span style=\"background-color: #f0fff0\">────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">│ {                                                                                                       │</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">│     'generated_sql': 'SELECT e.name\\nFROM employees e\\nJOIN (\\n    SELECT department_id, MAX(salary) AS │</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">│ max_salary\\n    FROM employees\\n    GROUP BY department_id\\n) AS max_salaries\\nON e.department_id =     │</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">│ max_salaries.department_id AND e.salary = max_salaries.max_salary;'                                     │</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">│ }                                                                                                       │</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great job adding in robust guardrails, next we can now focus on making sure our system performs at peak performance. Let us try improving the results of the model using Prompt Engineering techniques making sure that we compare results from the baseline with the new improvements. Feel free to go wild and experiment with all kinds of techniques for this section!\n",
        "\n",
        "## TODO: Evaluation Driven Development(EDD)\n",
        "\n",
        "EDD is similar to [Test Driven Development(TDD)](https://martinfowler.com/bliki/TestDrivenDevelopment.html) except we apply it to building LLM-powered applications, where our test cases in this case are derived from the Golden dataset that we created earlier and the goal here is to iteratively ensure that our LLM application can achieve a **100% pass rate** across all the examples.\n",
        "\n",
        "The main idea here is that if we currently have a 60% pass rate on the exact match metric, we should take an iterative approach towards increasing the pass rate. For example, the LLM could be failing on the following query `Display the department that has the highest average employee salary.`. In this section we will try to tune the prompt or other parts to ensure that the model is able to correctly generate the ground truth for it.\n",
        "\n",
        "NOTE: As we mentioned before some items from the dataset could result in false negatives since multiple implementations are possible for the same query.\n",
        "\n",
        "Here are a couple of things we can try out:\n",
        "- Try out Few-Shot Prompting and [other techniques](https://www.promptingguide.ai/techniques) and see how it impacts our evaluation performance.\n",
        "- Try tuning the parameters used for a model.\n",
        "- Try out different LLM models from OpenAI, Anthropic, etc. if you have API keys for them.\n",
        "\n",
        "You will have to setup an EvaluationDataset per each comparison since each run would have a different `actual_output` used within the LLMTestCase.\n",
        "```python\n",
        "from deepeval.evaluate import aggregate_metric_pass_rates\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "\n",
        "dataset_run_1 = EvaluationDataset(test_cases=[...])\n",
        "dataset_run_2 = EvaluationDataset(test_cases=[...])\n",
        "\n",
        "evaluation_results_run_1 = evaluate(dataset_run_1, [exact_match_metric])\n",
        "pass_rate_run_1 = aggregate_metric_pass_rates(evaluation_results_run_1.test_results)\n",
        "\n",
        "evaluation_results_run_2 = evaluate(dataset_run_2, [exact_match_metric])\n",
        "pass_rate_run_2 = aggregate_metric_pass_rates(evaluation_results_run_2.test_results)\n",
        "\n",
        "assert pass_rate_run_2.get(\"ExactMatch\") > pass_rate_run_1.get(\"ExactMatch\"), \\\n",
        "  \"Run 2 does not perform better than Run 1\"\n",
        "```\n"
      ],
      "metadata": {
        "id": "VgWBsMZ-Ex96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################\n",
        "# START YOUR CODE HERE\n",
        "######################\n",
        "from deepeval.evaluate import aggregate_metric_pass_rates\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "\n",
        "ZERO_SHOT_PROMPT_TEMPLATE = \"\"\"\n",
        "Generate a valid SQL query for the following natural language instruction:\n",
        "\n",
        "Query: ${query}\n",
        "\n",
        "Only generate SQL code and nothing else.\n",
        "Do not include \"sql\" at the beggining of the query.\n",
        "DO not have line break.\n",
        "\"\"\"\n",
        "\n",
        "def generate_sql_code(llm, test_case_query: str) -> str:\n",
        "    prompt = ZERO_SHOT_PROMPT_TEMPLATE.format(\n",
        "      query=test_case_query\n",
        "    )\n",
        "\n",
        "    result = llm.generate([[HumanMessage(content=prompt)]])\n",
        "    test_case_actual_output = result.generations[0][0].text.strip()\n",
        "    return test_case_actual_output\n",
        "\n",
        "# Test 1 (temperature = 0)\n",
        "llm1 = ChatOpenAI(\n",
        "  model_name=MODEL_NAME,\n",
        "  temperature=0.0,\n",
        "  max_tokens=None\n",
        ")\n",
        "\n",
        "test_cases = []\n",
        "\n",
        "for index in range(len(df)):\n",
        "  test_case_input = df.iloc[index]\n",
        "  test_case_query = test_case_input[\"Query\"]\n",
        "  test_case_expected_output = test_case_input[\"Ground Truth\"]\n",
        "  test_case_actual_output = generate_sql_code(llm1, test_case_input[\"Query\"])\n",
        "\n",
        "  test_case = LLMTestCase(\n",
        "    input=test_case_query,\n",
        "    actual_output=test_case_actual_output,\n",
        "    expected_output=test_case_expected_output,\n",
        "    context = []\n",
        "  )\n",
        "  test_cases.append(test_case)\n",
        "\n",
        "dataset_run_1 = EvaluationDataset(test_cases=test_cases)\n",
        "\n",
        "# Test 2 (temperature = 1.5)\n",
        "llm2 = ChatOpenAI(\n",
        "  model_name=MODEL_NAME,\n",
        "  temperature=1.5,\n",
        "  max_tokens=None\n",
        ")\n",
        "\n",
        "test_cases = []\n",
        "for index in range(len(df)):\n",
        "  test_case_input = df.iloc[index]\n",
        "  test_case_query = test_case_input[\"Query\"]\n",
        "  test_case_expected_output = test_case_input[\"Ground Truth\"]\n",
        "  test_case_actual_output = generate_sql_code(llm2, test_case_input[\"Query\"])\n",
        "\n",
        "  test_case = LLMTestCase(\n",
        "    input=test_case_query,\n",
        "    actual_output=test_case_actual_output,\n",
        "    expected_output=test_case_expected_output,\n",
        "    context = []\n",
        "  )\n",
        "  test_cases.append(test_case)\n",
        "\n",
        "dataset_run_2 = EvaluationDataset(test_cases=test_cases)\n",
        "\n",
        "# Compare Test 1 and 2\n",
        "evaluation_results_run_1 = evaluate(dataset_run_1, [exact_match_metric], run_async=False)\n",
        "evaluation_results_run_2 = evaluate(dataset_run_2, [exact_match_metric], run_async=False)\n",
        "\n",
        "pass_rate_run_1 = aggregate_metric_pass_rates(evaluation_results_run_1.test_results)\n",
        "pass_rate_run_2 = aggregate_metric_pass_rates(evaluation_results_run_2.test_results)\n",
        "\n",
        "print(pass_rate_run_1)\n",
        "print(pass_rate_run_2)\n",
        "\n",
        "assert pass_rate_run_2.get(\"ExactMatch\") > pass_rate_run_1.get(\"ExactMatch\"), \\\n",
        "  \"Run 2 does not perform better than Run 1\""
      ],
      "metadata": {
        "id": "fBl8t9QZNovv",
        "outputId": "2c9ed303-1fd4-495b-8e39-2fc77dbedf31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mExactMatch Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing \u001b[0m\u001b[3;38;2;55;65;81mNone\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">ExactMatch Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using </span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">None</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating 15 test case(s) sequentially: |██████████|100% (15/15) [Time Taken: 00:00, 60.06test case/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of employees.\n",
            "  - actual output: SELECT COUNT(*) FROM employees;\n",
            "  - expected output: SELECT COUNT(*) AS total_employees FROM employees;\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ ExactMatch (score: 1.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the highest salary in the Finance department.\n",
            "  - actual output: SELECT MAX(salary) FROM employees WHERE department = 'Finance';\n",
            "  - expected output: SELECT MAX(salary) FROM employees WHERE department = 'Finance';\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Get the average age of all managers.\n",
            "  - actual output: SELECT AVG(age) FROM employees WHERE position = 'manager';\n",
            "  - expected output: SELECT AVG(age) FROM employees WHERE position = 'Manager';\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List the names and emails of staff in the IT department.\n",
            "  - actual output: SELECT name, email FROM staff WHERE department = 'IT';\n",
            "  - expected output: SELECT name, email FROM employees WHERE department = 'IT';\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ ExactMatch (score: 1.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: What are the titles of the top 5 selling books?\n",
            "  - actual output: SELECT title FROM books ORDER BY sales DESC LIMIT 5;\n",
            "  - expected output: SELECT title FROM books ORDER BY sales DESC LIMIT 5;\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ ExactMatch (score: 1.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Which product has the least quantity in stock?\n",
            "  - actual output: SELECT product_name FROM products ORDER BY quantity_in_stock ASC LIMIT 1;\n",
            "  - expected output: SELECT product_name FROM products ORDER BY quantity_in_stock ASC LIMIT 1;\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ ExactMatch (score: 1.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Display the second highest salary in the organization.\n",
            "  - actual output: SELECT MAX(salary) FROM employees WHERE salary < (SELECT MAX(salary) FROM employees);\n",
            "  - expected output: SELECT MAX(salary) FROM employees WHERE salary < (SELECT MAX(salary) FROM employees);\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List employees who joined after 2015 and work in the Sales department.\n",
            "  - actual output: SELECT * FROM employees WHERE join_date > '2015-01-01' AND department = 'Sales';\n",
            "  - expected output: SELECT * FROM employees WHERE year(joined_date) > 2015 AND department = 'Sales';\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the average order value for each customer.\n",
            "  - actual output: SELECT customer_id, AVG(order_value) AS average_order_value FROM orders GROUP BY customer_id;\n",
            "  - expected output: SELECT customer_id, AVG(order_value) FROM orders GROUP BY customer_id;\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show departments that have more than 10 employees.\n",
            "  - actual output: SELECT department_name FROM departments GROUP BY department_name HAVING COUNT(employee_id) > 10;\n",
            "  - expected output: SELECT department FROM employees GROUP BY department HAVING COUNT(*) > 10;\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ ExactMatch (score: 1.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List all products that have never been ordered.\n",
            "  - actual output: SELECT * FROM products WHERE product_id NOT IN (SELECT product_id FROM orders);\n",
            "  - expected output: SELECT * FROM products WHERE product_id NOT IN (SELECT product_id FROM orders);\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Which customers have spent more than $1000 in total?\n",
            "  - actual output: SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(amount) > 1000;\n",
            "  - expected output: SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(order_value) > 1000;\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of orders placed each day last week.\n",
            "  - actual output: SELECT DATE(order_date) AS order_day, COUNT(*) AS total_orders FROM orders WHERE order_date >= CURDATE() - INTERVAL 7 DAY AND order_date < CURDATE() GROUP BY order_day;\n",
            "  - expected output: SELECT order_date, COUNT(*) FROM orders WHERE order_date > CURRENT_DATE - INTERVAL '7 days' GROUP BY order_date;\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List the names of employees who do not manage anyone.\n",
            "  - actual output: SELECT name FROM employees WHERE id NOT IN (SELECT manager_id FROM employees WHERE manager_id IS NOT NULL);\n",
            "  - expected output: SELECT name FROM employees WHERE name NOT IN (SELECT manager_name FROM employees);\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Display the department that has the highest average employee salary.\n",
            "  - actual output: SELECT department, AVG(salary) AS average_salary FROM employees GROUP BY department ORDER BY average_salary DESC LIMIT 1;\n",
            "  - expected output: SELECT department FROM employees GROUP BY department ORDER BY AVG(salary) DESC LIMIT 1;\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "ExactMatch: 33.33% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mExactMatch Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing \u001b[0m\u001b[3;38;2;55;65;81mNone\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">ExactMatch Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using </span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">None</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating 15 test case(s) sequentially: |██████████|100% (15/15) [Time Taken: 00:00, 50.19test case/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ ExactMatch (score: 1.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of employees.\n",
            "  - actual output: SELECT COUNT(*) AS total_employees FROM employees;\n",
            "  - expected output: SELECT COUNT(*) AS total_employees FROM employees;\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ ExactMatch (score: 1.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the highest salary in the Finance department.\n",
            "  - actual output: SELECT MAX(salary) FROM employees WHERE department = 'Finance';\n",
            "  - expected output: SELECT MAX(salary) FROM employees WHERE department = 'Finance';\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Get the average age of all managers.\n",
            "  - actual output: SELECT AVG(age) FROM employees WHERE role = 'manager';\n",
            "  - expected output: SELECT AVG(age) FROM employees WHERE position = 'Manager';\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List the names and emails of staff in the IT department.\n",
            "  - actual output: SELECT name, email FROM staff WHERE department = 'IT';\n",
            "  - expected output: SELECT name, email FROM employees WHERE department = 'IT';\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ ExactMatch (score: 1.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: What are the titles of the top 5 selling books?\n",
            "  - actual output: SELECT title FROM books ORDER BY sales DESC LIMIT 5;\n",
            "  - expected output: SELECT title FROM books ORDER BY sales DESC LIMIT 5;\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ ExactMatch (score: 1.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Which product has the least quantity in stock?\n",
            "  - actual output: SELECT product_name FROM products ORDER BY quantity_in_stock ASC LIMIT 1;\n",
            "  - expected output: SELECT product_name FROM products ORDER BY quantity_in_stock ASC LIMIT 1;\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ ExactMatch (score: 1.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Display the second highest salary in the organization.\n",
            "  - actual output: SELECT MAX(salary) FROM employees WHERE salary < (SELECT MAX(salary) FROM employees);\n",
            "  - expected output: SELECT MAX(salary) FROM employees WHERE salary < (SELECT MAX(salary) FROM employees);\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List employees who joined after 2015 and work in the Sales department.\n",
            "  - actual output: SELECT * FROM employees WHERE joining_date > '2015-01-01' AND department = 'Sales';\n",
            "  - expected output: SELECT * FROM employees WHERE year(joined_date) > 2015 AND department = 'Sales';\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the average order value for each customer.\n",
            "  - actual output: SELECT customer_id, AVG(order_value) AS average_order_value FROM orders GROUP BY customer_id;\n",
            "  - expected output: SELECT customer_id, AVG(order_value) FROM orders GROUP BY customer_id;\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show departments that have more than 10 employees.\n",
            "  - actual output: SELECT department_name FROM departments WHERE employee_count > 10;\n",
            "  - expected output: SELECT department FROM employees GROUP BY department HAVING COUNT(*) > 10;\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ ExactMatch (score: 1.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List all products that have never been ordered.\n",
            "  - actual output: SELECT * FROM products WHERE product_id NOT IN (SELECT product_id FROM orders);\n",
            "  - expected output: SELECT * FROM products WHERE product_id NOT IN (SELECT product_id FROM orders);\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Which customers have spent more than $1000 in total?\n",
            "  - actual output: SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(total_amount) > 1000;\n",
            "  - expected output: SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(order_value) > 1000;\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of orders placed each day last week.\n",
            "  - actual output: SELECT DATE(order_date) AS order_day, COUNT(*) AS total_orders FROM orders WHERE order_date >= CURDATE() - INTERVAL 7 DAY AND order_date < CURDATE() GROUP BY DATE(order_date);\n",
            "  - expected output: SELECT order_date, COUNT(*) FROM orders WHERE order_date > CURRENT_DATE - INTERVAL '7 days' GROUP BY order_date;\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List the names of employees who do not manage anyone.\n",
            "  - actual output: SELECT name FROM employees WHERE employee_id NOT IN (SELECT manager_id FROM employees WHERE manager_id IS NOT NULL);\n",
            "  - expected output: SELECT name FROM employees WHERE name NOT IN (SELECT manager_name FROM employees);\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ ExactMatch (score: 1.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Display the department that has the highest average employee salary.\n",
            "  - actual output: SELECT department FROM employees GROUP BY department ORDER BY AVG(salary) DESC LIMIT 1;\n",
            "  - expected output: SELECT department FROM employees GROUP BY department ORDER BY AVG(salary) DESC LIMIT 1;\n",
            "  - context: []\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "ExactMatch: 46.67% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "ExactMatch: 33.33% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "ExactMatch: 46.67% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\u001b[32m'ExactMatch'\u001b[0m: \u001b[1;36m0.3333333333333333\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'ExactMatch'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3333333333333333</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\u001b[32m'ExactMatch'\u001b[0m: \u001b[1;36m0.4666666666666667\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'ExactMatch'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4666666666666667</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Evaluating SQL Code Using a Stronger LLM\n",
        "\n",
        "Usually we would prioritise latency during inference using LLMs for most use cases, LLMs such as `gpt3.5-turbo` are much faster than `gpt4` variants while performing worse in general across benchmarks. What if we could also use a bigger and stronger LLM to aid with evaluations as well? There has been a rise of techniques doing just that where a smaller and faster LLM is evaluated by a bigger and stronger LLM. This process helps automate the evaluation part of any LLM-powered application! This is kinda of like having a dream within a dream just like in the movie *Inception*.\n",
        "\n",
        "![Dream within a dream from Inception](https://drive.google.com/uc?id=1EC7rNg_LCLjIJ9SRyQpGVgtplOHZllDm)\n",
        "\n",
        "\n",
        "**NOTE:** LLMs are very powerful and can help automate so many parts of the process but becareful not to be too reliant on them, at the end of the day they aren't perfect and hallucinate a lot. Therefore, its vital that you always keep a human-in-the-loop to verify things as a final seal of approval.\n",
        "\n",
        "\n",
        "### Introducing G-Eval\n",
        "\n",
        "[G-Eval](https://arxiv.org/abs/2303.16634) is a method that utilizes LLMs alongside a Chain-of-Thought (CoT) and form-filling approach to assess the outputs of LLMs. Initially, a task outline and evaluation parameters are provided to an LLM, which is then requested to create a CoT detailing the evaluation steps. For assessing the coherence of news summaries, the prompt, CoT, news article, and summary are merged, and the LLM is instructed to produce a score ranging from 1 to 5. Subsequently, the output token probabilities generated by the LLM are used to standardize this score, with a weighted summation taken as the ultimate outcome.\n",
        "\n",
        "It was observed that using GPT-4 as an evaluator resulted in a high Spearman correlation with human assessments (0.514), surpassing all former methods. In the realm of summarization, it exceeded all prior state-of-the-art (SOTA) evaluators in the SummEval benchmark, which evaluates coherence, consistency, fluency, and relevance.\n",
        "\n",
        "\n",
        "![G-Eval](https://drive.google.com/uc?id=1dq1BAjLY_Es5r-UiD96ZnFlxWTYcNv8g)\n",
        "\n",
        "deepeval already comes out of the box with support for using G-Eval [here](https://docs.confident-ai.com/docs/metrics-llm-evals) where you can very easily just create a new metric and run it against your EvaluationDataset:\n",
        "\n",
        "```python\n",
        "from deepeval.metrics import GEval\n",
        "from deepeval.test_case import LLMTestCaseParams\n",
        "\n",
        "correctness_metric = GEval(\n",
        "    name=\"Correctness\",\n",
        "    criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n",
        "    # NOTE: you can only provide either criteria or evaluation_steps, and not both\n",
        "    evaluation_steps=[\n",
        "        \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
        "        \"You should also heavily penalize omission of detail\",\n",
        "        \"Vague language, or contradicting OPINIONS, are OK\"\n",
        "    ],\n",
        "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
        ")\n",
        "```\n",
        "Try creating a G-Eval metric for SQL Code Generation and running it through our EvaluationDataset like before.\n",
        "\n",
        "\n",
        "**NOTE:** The API Key provided to you only allows access to `gpt-4o-mini`, we can still use it either way but in the real world you would ideally use a more powerful model for this task. If you were to use `gpt-4o` as your base model then you probably don't have another *stronger* LLM to use therefore this also mimics that kind of constraint."
      ],
      "metadata": {
        "id": "pnh0hFV-HLK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################\n",
        "# START YOUR CODE HERE\n",
        "######################\n",
        "from deepeval.metrics import GEval\n",
        "from deepeval.test_case import LLMTestCaseParams\n",
        "\n",
        "correctness_metric = GEval(\n",
        "    model=\"gpt-4o\",\n",
        "    name=\"Correctness\",\n",
        "    criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n",
        "    # NOTE: you can only provide either criteria or evaluation_steps, and not both\n",
        "    evaluation_steps=[\n",
        "        \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
        "        \"You should also heavily penalize omission of detail\",\n",
        "        \"Vague language, or contradicting OPINIONS, are OK\"\n",
        "    ],\n",
        "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
        ")\n",
        "\n",
        "correctness_metric.measure(test_case)\n",
        "print(correctness_metric.score)\n",
        "print(correctness_metric.reason)\n"
      ],
      "metadata": {
        "id": "VgazsrdbRouA",
        "outputId": "bc27faf2-66ad-4188-91e6-5b0513c833a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "8f9839ac8bd844bd99bdc9a43c03b3ab",
            "76ce4532240045e4b1716d66cc915fe8"
          ]
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f9839ac8bd844bd99bdc9a43c03b3ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m0.8879127833834186\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8879127833834186</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "The SQL query correctly identifies the department with the highest average salary using GROUP BY and ORDER BY \n",
              "\u001b[1;35mAVG\u001b[0m\u001b[1m(\u001b[0msalary\u001b[1m)\u001b[0m DESC, fulfilling the input requirement. It omits selecting the average value itself, but this detail is\n",
              "not explicitly required in the input.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The SQL query correctly identifies the department with the highest average salary using GROUP BY and ORDER BY \n",
              "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AVG</span><span style=\"font-weight: bold\">(</span>salary<span style=\"font-weight: bold\">)</span> DESC, fulfilling the input requirement. It omits selecting the average value itself, but this detail is\n",
              "not explicitly required in the input.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great job on getting G-Eval to work with our dataset! What do you think about using LLMs to evaluate another LLM vs just using traditional Machine Learning metrics? What are the Pros/Cons of it and where do you think it would work better or worse in?\n",
        "\n",
        "**TODO: Add your thoughts here!**"
      ],
      "metadata": {
        "id": "HC6hxGkHRuEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Extra Credit: Optional Tasks\n",
        "\n",
        "- Try out implementing the Execution Score from the Snowflake team and comparing the results of it to Exact Match Metric and G-Eval, this [blog](https://medium.com/snowflake/inside-snowflake-building-the-most-powerful-sql-llm-in-the-world-1a33b3ee0d37) from the Snowflake Team covers how you can use LLMs to assists with evaluation.\n",
        "- If you use SQL enough you would know that there are multiple ways in which we can answer a particular query especially if you had to update your ground truth earlier. With that you can try out other frameworks such as [sql-eval](https://github.com/defog-ai/sql-eval) which provides mo like we did before but more importanly also includes code execution into context when evaluating LLMs.\n",
        "- Guardrails do not just apply to the output from an LLM, instead we can also apply them to the actual input that a user feeds into the LLM. You can try out **Detecting Toxicity**, **Restricting the input to just be related to SQL code generation**, **Limiting the amount of tokens used**, etc.\n",
        "- Our BASIC_GROUND_TRUTH_DATASET is quite small, see how you can add in more examples using your own domain knowledge or you could even leverage an LLM to do so via synthetic data generation which is supported within deepeval [here](https://docs.confident-ai.com/docs/evaluation-datasets-synthetic-data). Remember not all users will put in queries that can be answered with SQL code. For example, if the user asks `Who will win the next US Presidential Election?` you definitely don't want to answer it at all!\n"
      ],
      "metadata": {
        "id": "UL-XI8PiNU1z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dIcvjLeVSEeD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}